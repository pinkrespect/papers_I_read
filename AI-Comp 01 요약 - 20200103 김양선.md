Deep Compression:\
Compressing Deep Neural Networks with Pruning, Trained Quantization and
Huffman coding

1.  Abstract

2.  Introduction

3.  Network Pruning

4.  Trained Quantization and Weight Sharing

    A.  Weight Sharing

    B.  Initialization of Shared Weights

    C.  Feed-forward and Back-propagation

5.  Huffman Coding

6.  Discussion

    A.  Pruning and Quantization working together

    B.  Centroid Initialization

    C.  Speed up and Energy Efficiency

    D.  Ratio of Weights, Index and Codebook

7.  Related Work

8.  Future Work

9.  Conclusion

```{=html}
<!-- -->
```
1.  Abstract

-   Deep Compression

    -   신경망은 컴퓨팅 집약적이고 메모리 집약적이어서 하드웨어 자원이
        제한된 임베디드 시스템에 배치하기가 어려움

        -   한계에 대처하기 위해, 우리는 세 단계의 파이프라인인 \"딥
            압축\"을 도입

            -   Pruning, Trained Quantization, Huffman Coding

        -   이 파이프라인의 정확도에 영향을 미치지 않고 신경 네트워크의
            저장 요건을 35배에서 49배까지 감소

    -   Pruning

        -   우선 중요한 연결만 배워서 네트워크를 자름

        -   프루닝은 연결 수를 9배에서 13배까지 감소시킴

    -   Trained Quantization

        -   가중치 분담을 시행하기 위해 가중치를 정량화(Quantization)

        -   마지막으로, Huffman Coding을 적용

        -   정량화하면 각 연결을 나타내는 비트 수가 32개에서 5개로
            줄어듬

    -   처음 두 단계 후에 우리는 남은 연결과 Quantized Centroid를
        미세하게 조정하기 위해 네트워크를 재교육

    -   이를 통해 모델을 오프칩 DRAM 메모리가 아닌 온칩 SRAM 캐시에
        장착할 수 있음

    -   이 압축 방법은 애플리케이션 크기와 다운로드 대역폭이 제한되는
        모바일 애플리케이션에서 복잡한 신경 네트워크의 사용을 용이하게
        함

    -   CPU, GPU, 모바일 GPU에 벤치마킹된 압축 네트워크는 3배에서
        4배까지 속도를 높이고 에너지 효율을 3배에서 7배까지 향상

2.  Introduction

    A.  딥 러닝의 한계

        i.  모바일로 구동되는 심층신경망은 프라이버시 향상, 네트워크
            대역폭 감소, 실시간 처리 등 많은 훌륭한 기능을 가지고
            있지만, 큰 스토리지 오버헤드는 딥 뉴럴 네트워크가 모바일
            앱에 통합되는 것을 막음

        ii. 상당한 에너지를 소모하는 큰 신경망을 가동하려면 많은 메모리
            대역폭이 필요하고, 제품을 가져오려면 많은 계산이 필요하지만
            모바일 기기는 배터리 제약을 받아 심층 신경망과 같은 전력
            소모 애플리케이션이 배치되기 어려움

        -   에너지 소비는 메모리 접근과 깊게 관련되어 있으며, 현재의
            기술로는 모바일 기술에 접목이 어려움

    B.  이 논문에서의 목표

        i.  대규모 네트워크에서 추론하는 데 필요한 스토리지와 에너지를
            줄여 모바일 장치에 배치할 수 있도록 하는 것

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image1.png){width="6.26875in" height="2.1868055555555554in"}

-   신경망에 필요한 저장량을 원래의 정확도를 보존하며 줄이기 위한 3단계
    파이프라인(그림 1)인 \"딥 컴프레션\"을 제시

-   중복 연결을 제거하여 가장 유용한 연결만 유지하여 네트워킹을 Pruning

-   가중치를 정량화하여 복수의 연결부가 동일한 가중치를 공유하므로
    코드북(유효 가중치)과 지수만 저장하도록 함

-   마지막으로 유효 가중치의 편중 분포를 이용하기 위해 허프먼 코딩을
    적용

ii. 주요 아이디어

-   가지치기 및 훈련된 정량화가 서로 간섭하지 않고 네트워크를 압축 가능

    -   높은 압축률을 이끌어 낼 수 있음

-   에너지 소비형 D램을 오프칩으로 만드는 대신 칩에 모든 가중치를 캐싱할
    수 있을 정도의 저용량(몇 메가바이트)

3.  Network Pruning

    A.  네트워크 프루닝은 CNN 모델을 압축하기 위해 널리 연구되어 옴

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image2.png){width="6.26875in" height="2.216666666666667in"}

B.  프루닝 방법

    i.  그림 1의 왼쪽에서 볼 수 있듯이, 우리는 정상적인 네트워크 훈련을
        통해 연결성을 배우는 것으로 시작

    ii. 다음으로, 우리는 작은 무게의 연결부를 잘라냄

    -   임계값보다 낮은 가중치를 가진 모든 연결부는 네트워크에서 제거

    iii. 마지막으로, 우리는 남은 희박 연결에 대한 최종 가중치를 배우도록
         네트워크를 재교육

![](media/image3.png){width="6.26875in" height="0.9402777777777778in"}

iv. 더 압축하기 위해 절대 위치 대신 인덱스를 저장하고, 이 차이를 Conv
    레이어의 경우 8비트, fc 레이어의 경우 5비트로 인코딩

v.  bound보다 큰 인덱스 차이가 필요한 경우 그림 2에 표시된 제로 패딩
    솔루션으로, 차이가 8을 초과할 경우 가장 큰 3비트(예시처럼)를 필러
    0을 추가

```{=html}
<!-- -->
```
4.  Trained Quantization and Weight Sharing

-   네트워크 정량화 및 가중치 공유

    -   각 가중치를 나타내는 데 필요한 비트 수를 줄임으로써 제거된
        네트워크를 더욱 압축

    -   여러 개의 연결부가 동일한 무게를 공유하도록 함으로써 저장해야
        하는 유효 가중치의 수를 제한, 공유 가중치를 미세 조정

![](media/image4.png){width="6.26875in" height="2.6868055555555554in"}

-   가중치 공유는 그림 3 참고

    -   우리가 4개의 입력 뉴런과 4개의 출력 뉴런을 가진 층을 가지고
        있다고 가정하면, 가중치는 4 × 4 매트릭스

    -   왼쪽 상단에는 4 × 4 가중치 행렬이 있고, 왼쪽 하단에는 4 × 4 경사
        행렬

    -   가중치는 4개의 빈(색상 4개로 표시됨)으로 정량화되며, 동일한
        bins의 모든 가중치는 동일한 값을 공유하므로, 각 가중치에 대해
        작은 지수만 공유 가중치 표에 저장

    -   업데이트하는 동안 모든 그라데이션은 색상으로 그룹화되고 함께
        합산되며, 학습 속도를 곱한 후 마지막 반복에서 공유된 중심에서
        제거

-   k 클러스터가 주어진 압축률을 계산하려면 인덱스를 인코딩할 log2(k)
    비트가 필요

-   일반적으로 n개의 연결부가 있고 각 연결부가 b비트로 표현되는 경우, k
    공유 가중치만 가지도록 연결을 제한하면 다음과 같은 압축률이 발생

![](media/image5.png){width="6.26875in" height="0.3958333333333333in"}

A.  Weight Sharing

-   k-평균 클러스터링을 사용하여 훈련된 네트워크의 각 계층에 대한 공유
    가중치를 식별, 동일한 클러스터에 속하는 모든 가중치가 동일한
    가중치를 공유하도록 함

-   무게는 여러 층에 걸쳐 공유되지는 않음

-   파티션 N에 대하여 원래 가중치 W = {w1, w2, \..., wn}를 k 클러스터 C
    = {c1, c2, \..., ck}, n ≫ k로 분할하여 클러스터 내 제곱합(WCSS)을
    최소화하면:

![개체이(가) 표시된 사진 자동 생성된
설명](media/image6.png){width="6.26875in" height="0.5222222222222223in"}

-   네트워크가 완전히 훈련된 후에 가중치 공유를 결정하여 공유된 가중치가
    원래 네트워크에 접근하도록 함

B.  Initialization of Shared Weights

-   중심 초기화

    -   중심을 초기화하는 것은 클러스터링 품질에 영향을 미침

        -   따라서 네트워크의 예측 정확도에 영향을 미침

    -   세가지 초기화 방법

![지도이(가) 표시된 사진 자동 생성된
설명](media/image7.png){width="6.26875in" height="1.9256944444444444in"}

-   Forgy(랜덤) 초기화

    -   데이터 세트에서 k 관측치를 무작위로 선택, 이를 초기 중심선으로
        사용

    -   초기화된 중심체는 노란색으로 표시

    -   이항 분포에는 두 개의 봉우리가 있기 때문에 포기 방법은 이 두
        봉우리 주위에 집중하는 경향이 있다.

-   밀도 기반 초기화

    -   무게의 CDF를 Y축에 선형 공간화한 다음 CDF와 수평 교차점을 찾은
        다음, 파란색 점과 같이 중심점이 되는 X축에서 수직 교차점 확인

    -   두 봉우리 둘레를 중심으로 중심부를 더 촘촘하게 만들지만 Forgy
        방법보다는 더 스크래치하게 만듬

-   선형 초기화

    -   원래 중량의 \[min, max\] 사이에 중심체를 선형 공간화

    -   가중치의 분포에 불변하며, 앞의 두 가지 방법에 비해 가장 많이
        산란

C.  Feed-forward and Back-propagation

-   1차원 k-평균 군집화의 중심은 공유 가중치들이다. 무게 테이블을
    올려다보는 Feed-forward 방식과 Back-propagation 단계 중 한 가지 방향
    전환 단계가 있다. 각 연결에 대해 공유 가중치 테이블에 대한 인덱스가
    저장된다. Back-propagation 시 각 공유 중량에 대한 gradient를
    계산하여 공유 중량을 업데이트하는 데 사용한다. 이 절차는 그림 3과
    같다.

-   우리는 L에 의한 손실, ith column의 무게, 그리고 Wij에 의한 j번째
    열의 중심 지수, Iij에 의한 원소의 중심 지수, Ck에 의한 층의 k번째
    중심 지수를 나타낸다. 지시계 함수 1()을 사용하여 중심선의 기울기를
    다음과 같이 계산한다.

![개체이(가) 표시된 사진 자동 생성된
설명](media/image8.png){width="6.26875in" height="0.4777777777777778in"}

5.  Huffman Coding

-   허프만 코드

    -   손실 없는 데이터 압축에 일반적으로 사용되는 최적의 Prefix 코드

    -   소스 기호를 인코딩하기 위해 가변 길이 Codewords를 사용

    -   표는 각 기호의 발생 확률에서 도출

    -   더 일반적인 기호는 더 적은 비트로 표현

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image9.png){width="6.261111111111111in"
height="1.836111111111111in"}

-   정량화된 가중치의 확률 분포와 AlexNet에서 마지막으로 완전히 연결된
    계층의 희박한 행렬 지수

    -   두 분포 모두 편향

        -   정량화된 가중치의 대부분은 두 피크 주위에 분포

    -   희박한 행렬 지수의 차이는 20보다 적으

    -   Huffman 코딩으로 균일하지 않게 분산된 값을 사용하면 네트워크
        스토리지의 20% - 30%를 절약 가능

6.  Discussion

    A.  Pruning and Quantization working together

![텍스트, 지도이(가) 표시된 사진 자동 생성된
설명](media/image10.png){width="6.261111111111111in"
height="2.8805555555555555in"}

-   개별적으로 자르고 정량화 하기 위한 서로 다른 압축률의 정확도

    -   보라색과 노란색 선

        -   개별적으로 작업할 때, 자른 네트워크의 정확도는 원래 크기의
            8% 미만으로 압축할 때 현저하게 떨어지기 시작

        -   정량화된 네트워크의 정확도 또한 원래 크기의 8% 미만으로
            압축할 때 현저하게 떨어지기 시작

    -   적색 선에서 보듯이 결합하면 네트워크는 정확도 손실 없이 원래
        크기의 3%로 압축

    -   오른쪽 끝에 SVD의 결과는 코스트가 저지만 압축률이 낮은 것을 비교

![텍스트, 지도이(가) 표시된 사진 자동 생성된
설명](media/image11.png){width="6.261111111111111in"
height="1.4326388888888888in"}

-   CONV 레이어(왼쪽), FC 레이어(가운데) 및 모든 레이어(오른쪽)

    -   연결 당 비트 수가 적은 상태에서 정확도가 떨어짐

    -   각 플롯은 상위 1과 상위 5의 정확도를 모두 보여줌

        -   점선은 정량화만 적용했지만 프루닝은 적용하지 않음

        -   실선은 정량화와 자르기를 동시에 수행

        -   그 둘 사이에는 차이가 거의 없음

        -   프루닝과 정량화는 잘 통한다는 것을 보여줌

-   정량화

    -   프루닝 네트워크에서 잘 작동

        -   프루닝을 안 한 알렉스는 정량화할 6천만 개의 가중치를 가지고
            있는 반면, 프루닝을 한 알렉스넷은 정량화할 670만 개의
            가중치를 가지고 있음

        -   같은 양의 Centroid를 고려할 때, 후자의 오차는 적음

    A.  Centroid Initialization

![지도, 텍스트이(가) 표시된 사진 자동 생성된
설명](media/image12.png){width="6.261111111111111in" height="1.73125in"}

-   Top-1 정확도(왼쪽)와 Top-5 정확도(오른쪽)에 대해 세 가지 다른 초기화
    방법의 정확도를 비교

    -   네트워크는 x축에 표시된 것처럼 2\~8비트로 정량화

    -   선형 초기화는 3비트를 제외한 모든 경우에 밀도 초기화 및 무작위
        초기화를 능가

-   선형 초기화의 초기 중심

    -   최소값에서 최대값까지 x축에 균등하게 분포

        -   큰 무게가 작은 무게보다 더 중요한 역할을 하기 때문에 큰
            무게를 유지하는 데 도움이 되며, 이는 네트워크 프루닝에서도
            나타남

        -   무작위 또는 밀도 기반 초기화 모두 큰 중심체를 유지하지 못함

            -   이러한 초기화 방법으로 큰 무게는 큰 무게가 거의 없기
                때문에 작은 중심체까지 군집

            -   이와는 대조적으로, 선형 초기화는 큰 무게로 큰 중심점을
                형성할 수 있는 더 좋은 기회를 제공

    A.  Speed up and Energy Efficiency

-   Deep Compression은 모바일에서 실행되는 latency에 중점을 둔
    애플리케이션을 목표

    -   실시간 추론을 필요로 함

    -   Batch가 준비 되기를 기다리는 것은 Latency를 크게 증가 시킴

    -   성능과 에너지 효율을 벤치마킹할 때 배치 크기가 1인 경우를 고려

-   Fully-connected 레이어가 모델 크기(90% 이상)를 지배, 딥
    Compression(VGG-16에서 96%의 가중치 제거)에 의해 가장 많이 압축

    -   고속 R-CNN(Girshick, 2015)과 같은 최첨단 객체 감지
        알고리즘에서는 압축되지 않은 모델의 FC 레이어에서 최대 38%의
        연산 시간을 소비

    -   FC 레이어를 벤치마킹하면 확실한 차이를 알 수 있음

        -   AlexNet과 VGG-16의 FC6, FC7, FC8 레이어에 벤치마크를 설정

        -   비-batch의 경우, 활성화 매트릭스는 단 하나의 열만 있는
            벡터이므로, 계산은 오리지널/프루닝 모델에 대해 각각
            밀도/희석 매트릭스 벡터 곱셈으로 요약 가능

        -   현재 CPU와 GPU의 BLAS 라이브러리는 간접 조회 및 상대
            인덱싱을 지원하지 않기 때문에 정량화된 모델을 벤치마킹하지
            않음

-   NVIDIA GeForce GTX Titan X와 Intel Core i7 5930K를 데스크톱
    프로세서(NVIDIA Digits Dev Box와 동일한 패키지)로, NVIDIA Tegra K1을
    모바일 프로세서로 비교

    -   GPU에서 벤치마크를 실행하기 위해, CuB를 사용

        -   프루닝 된 희소층

            -   희소 행렬을 CSR 형식으로 저장하고, GPU에서 희소
                행렬-벡터 곱셈에 최적화된 큐 SPARSE CSRMV 커널을 사용

            -   CPU에서 벤치마크를 실행하기 위해서는 원래의 밀도
                모델에는 MKL CBLAS GEMV

            -   프루닝 된 희소 모델에는 MKL SPBLAS CSRMV를 사용

-   서로 다른 시스템 간의 전력 소비량을 비교하기 위해서는 일관된
    방식으로 전력을 측정하는 것이 중요

    -   분석을 위해 전체 애플리케이션 프로세서(AP) / SOC와 DRAM을 결합한
        사전 규제 전력을 비교

    -   CPU에서 벤치마크는 단일 Haswell-E 클래스 Core i7-5930K
        프로세서가 있는 단일 소켓에서 실행

    -   CPU 소켓과 DRAM 전력은 인텔이 제공하는 pcm-power 유틸리티에서
        보이는 대로

    -   GPU의 경우, nvidia-smi 유틸리티를 사용하여 타이탄 X의 전력을
        보고

    -   모바일 GPU의 경우, Jetson TK1 개발 보드를 사용하고, 전력계로 총
        전력 소비량을 측정

    -   우리는 Tegra K1에 대한 AP+DRAM 전력을 보고하기 위해 AC-DC 변환
        손실 15%, 규제 효율 85% 및 주변 부품(NVIDIA, a)에 의해 소비되는
        전력 15%를 가정

-   연산 특성 대비 메모리 액세스 비율

    -   입력 활성화가 매트릭스에 결합되면 계산은 매트릭스 매트릭스
        곱셈이 되며, 여기서 차단으로 지역성을 개선 가능

    -   매트릭스는 캐시에 맞도록 차단되고 효율적으로 재사용

    -   메모리 액세스량은 O(n2)이고, 연산량은 O(n3)이며, 메모리 액세스와
        연산 사이의 비율은 1/n의 순서

    -   일괄 처리가 허용되지 않는 경우

        -   실시간 처리에서 입력 활성화는 단일 벡터

        -   계산은 매트릭스-벡터 곱셈

        -   메모리 액세스량은 O(n\^2)

        -   연산량은 O(n\^2)

        -   메모리 액세스와 연산은 크기가 동일(1/n과 반대)

        -   MV가 MM보다 더 메모리 경계가 빠르다는 것을 의미

            -   따라서 메모리 공간을 줄이는 것은 비 배치 사례에 매우
                중요

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image13.png){width="6.261111111111111in"
height="1.4555555555555555in"}

-   그림 9는 다른 하드웨어의 가지치기 속도

    -   각 벤치마크에 대해 6개의 열이 있으며, 고밀도/삭제된 네트워크에서
        CPU/GPU/TK1의 계산 시간을 보여줌

    -   시간은 CPU로 정상화

    -   배치 크기가 1이면, 특히 캐시에 들어갈 수 없는 큰 매트릭스의 경우
        메모리 사용 공간이 더 작고 오버헤드를 완화하기 때문에 고밀도
        네트워크를 통해 평균 3배에서 4배의 속도를 얻을 수 있음

-   지연 시간 조정 애플리케이션

    -   일괄 처리는 메모리 인접성을 향상

    -   여기서 가중치를 차단하고 매트릭스 곱셈에서 재사용 가능

![](media/image14.png){width="6.261111111111111in"
height="1.4555555555555555in"}

-   그림 10은 다른 하드웨어의 가지치기 에너지 효율

    -   에너지 소비를 얻기 위해 전력 소비를 계산 시간으로 곱한 다음,
        에너지 효율을 얻기 위해 CPU로 표준화

        -   배치 크기가 1일 때, 잘린 네트워크 계층은 고밀도 네트워크에서
            평균 3배에서 7배 더 적은 에너지를 소비

        -   nvidia-smi에 의해 보고된 GPU 활용률은 밀도와 희박한 사례
            모두에 대해 99%

    A.  Ratio of Weights, Index and Codebook

-   프루닝은 중량 매트릭스를 희박하게 만들므로 0이 아닌 요소의 색인을
    저장하는 데 여분의 공간이 필요

    -   Quantization은 코드북의 저장 공간을 추가

![](media/image15.png){width="6.261111111111111in"
height="1.4555555555555555in"}

-   그림 11은 네 개의 네트워크를 정량화할 때 세 개의 서로 다른 요소들의
    분해를 보여줌

-   평균적으로 가중치와 희박한 색인은 모두 5비트로 암호화되기 때문에
    저장량은 대략 반반

-   코드북의 오버헤드는 매우 작고 무시 가능

7.  Future Work

-   프루닝 네트워크는 다양한 하드웨어에서 벤치마킹

    -   웨이트 공유가 있는 정량화된 네트워크는 기성 cuSPARSE 또는 MKL
        SPBLAS 라이브러리가 간접 매트릭스 진입 조회를 지원하지 않음

    -   CSC 또는 CSR 형식의 상대적 지수도 지원하지 않음

        -   캐시에 모형을 맞추는 Deep Compression의 완전한 이점은 완전히
            알 수 없음

-   소프트웨어 솔루션은 이를 지원하는 맞춤형 GPU 커널을 작성하는 것

    -   하드웨어 솔루션은 희박하고 정량화된 네트워크 구조를 통과하도록
        전문화된 맞춤형 ASIC 아키텍처를 구축하는 것

        -   맞춤형 정량화 비트 폭도 지원

        -   우리는 이 아키텍처가 오프칩 DRAM 액세스 대신 온칩 SRAM
            액세스에 의해 지배할 것

8.  Conclusion

-   정확도에 영향을 주지 않고 신경망을 압축한 \"딥 압축\"을 제시

-   중요하지 않은 연결을 잘라내고, 가중치 공유를 사용하여 네트워크를
    정량화 한 다음, 허프먼 코딩을 적용함으로써 작동

-   정확성 손실 없이 무게 저장량을 35배 줄인 알렉스넷 가능

-   VGG-16과 LeNet 네트워크에 대해 정확도 손실 없이 49× 39× 압축된
    유사한 결과를 보여줌

    -   이로 인해 모바일 앱에 cnnet을 넣어야 하는 스토리지 요구사항이
        줄어듬

    -   Deep Compression 후 이러한 네트워크 크기는 오프칩 DRAM
        메모리(640pJ/access)가 아닌 온칩 SRAM 캐시(5pJ/access)에 적합

    -   잠재적으로 깊은 신경 네트워크는 모바일로 작동하기에 더 에너지
        효율이 높아짐

    -   이 압축 방법은 또한 애플리케이션 크기와 다운로드 대역폭이
        제한되는 모바일 애플리케이션에서 복잡한 신경 네트워크의 사용을
        용이하게 만듬.
