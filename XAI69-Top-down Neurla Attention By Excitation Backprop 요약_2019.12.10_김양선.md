**Top-down neural attention by Excitation Backprop**

**목차**

1.  **Abstract(요약)**

2.  **Introduction(소개)**

3.  **Related Work(관련 작업들)**

4.  **Method(방법)**

    A.  Top-down Neural Attention based on Probabilistic WTA

    B.  Excitation Backprop

        i.  *The response of the activation neuron is non-negative*

        ii. *An activation neuron is tuned to detect certain visual
            features. Its response is positively correlated to its
            confidence of the detection.*

    C.  Contrastive Top-down Attention

    D.  Implementation of Excitation Backprop

        i.  *Local Response Normalization(LRN) Layer*

5.  **Experiments(실험)**

    A.  The Pointing Game

        i.  Evaluation Setting.

        ii. *Datasets.*

        iii. *CNN classifiers*

        iv. *Test methods.*

        v.  *Result.*

        vi. *Layer selection effects*

        vii. *Analysis of contrastive top-down attention*

    B.  Localizing Dominant Objects

        i.  *Dataset and evaluation*

        ii. *Results*

    C.  Text-to-Region Association

        i.  *Tag classifier training*

        ii. *Dataset and evaluation*

        iii. *Results*

6.  **Conclusion(결론)**
