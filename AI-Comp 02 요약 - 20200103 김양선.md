EIE:\
Efficient Inference Engine

on Compressed Deep Neural Network

1.  Abstract

2.  Introduction

3.  Motivation

4.  DNN Compression and Parallelization

    A.  Computation

    B.  Representation

    C.  Parallelizing Compressed DNN

5.  Hardware Implementation

    A.  Activation Queue and Load Balancing

    B.  Pointer Read Unit

    C.  Sparse Matrix Read Unit

    D.  Arithmetic Unit

    E.  Activation Read/Write

    F.  Distributed Leading Non-Zero Detection

    G.  Central Control Unit

6.  Evaluation Methodology

    A.  Simulator, RTL and Layout

    B.  Comparison Baseline

        i.  CPU

        ii. GPU

        iii. Mobile GPU

    C.  Benchmarks

7.  Experimental Results

    A.  Performance

    B.  Energy

    C.  Design Space Exploration

        i.  Queue depth

        ii. SRAM width

        iii. Arithmetic Precision

8.  Discussion

    A.  Workload Partitioning

    B.  Scalability

    C.  Flexibility

9.  Conclusion

```{=html}
<!-- -->
```
1.  Abstract

-   Deep Compression

    -   대형 DNN(AlexNet 및 VGGNet)을 온칩 SRAM에 완전히 장착할 수 있게
        해줌

    -   중복 연결부를 잘라내고 여러 연결부가 동일한 가중치를 공유하도록
        함으로써 달성

-   에너지 효율적 추론 엔진(EIE)을 제안

    -   압축 네트워크 모델에서 추론을 수행하고 가중치 공유로 결과적으로
        희소 행렬-벡터 곱셈을 가속화하는 엔진

    -   DRAM에서 SRAM으로 전환하면 EIE는 120배의 에너지를 절약 가능

    -   Exploiting sparsity는 10배의 에너지를 절약

    -   가중치 공유는 8배의 에너지를 절약

    -   ReLU에서 제로 활성화를 건너뛰면 3배의 에너지를 절약

    -   9개의 DNN 벤치마크에서 평가

        -   EIE는 압축하지 않은 동일한 DNN의 CPU 및 GPU 구현에 비해
            189배, 13배 빠름

        -   압축된 네트워크에서 직접 작동하는 102 GOPS의 처리 능력

        -   압축되지 않은 네트워크에서 3 TOPS에 해당

        -   600mW의 전력 소산으로 알렉스넷의 FC 레이어를 1.88×104
            프레임/초로 처리

        -   CPU와 GPU보다 각각 2만4000배, 3,400배 높은 에너지 효율을
            자랑

        -   DaDianNao에 비해 처리량 2.9배, 19배, 3배, 에너지 효율과 면적
            효율 높음

2.  Introduction

-   신경 네트워크는 컴퓨터 비전, 음성 인식, 자연어 처리 등에서 보편화 됨

-   대형 DNN 모델

    -   강력하지만 외부 D램에 저장해야 하고, 각 이미지, 단어 또는 음성
        샘플에 대해 매번 가져와야 하기 때문에 많은 양의 에너지를 소비

    -   임베디드 모바일 애플리케이션의 경우 이러한 리소스 요구는
        엄청나게 커짐

    -   데이터 재사용이 없을 경우 필요한 메모리 접근에 의해 총 에너지가
        지배됨

    -   대형 네트워크는 온칩스토리지에 맞지 않기 때문에 더 많은 비용이
        드는 D램 액세스가 필요

-   CNN의 데이터 재사용과 조작

    -   맞춤형 하드웨어에 매우 적합, CNN의 효율적인 실행이 집중적으로
        연구

        -   RNN과 LSTM에서 널리 사용되는 Fully-Connected(FC) 계층은 대형
            네트워크에서는 대역폭이 제한

    -   CONV 레이어와는 달리 FC 레이어에서는 파라미터 재사용이 없음

    -   데이터 일괄화는 CPU나 GPU에서 네트워크를 교육할 때 효율적인
        솔루션이 되었지만, 지연 시간이 요구되는 실시간 애플리케이션에는
        적합하지 않음

-   프루닝 및 무게분담을 통한 네트워크 압축

    -   AlexNet(60M 파라미터, 240MB) 및 VGG-16(130M 파라미터, 520MB)과
        같은 현대적 네트워크를 온칩 SRAM에 맞출 수 있음

        -   이러한 압축된 모델을 처리하는 것은 어려움

        -   가지치기하면 매트릭스는 희박해지고 지표는 상대적이 됨

        -   가중치 분담을 통해 각 무게에 대해 짧은 (4비트) 지수만을 저장

        -   이것은 CPU와 GPU에 복잡성과 비효율성을 야기하는 추가적인
            수준의 인디렉션을 추가

-   EIE의 제안

    -   효율적인 맞춤형 추론 엔진

    -   맞춤형 sparse 매트릭스 벡터 곱셈을 수행, 효율 손실 없이 가중치
        공유를 처리하는 전문화된 가속기

    -   EIE는 확장 가능한 PE(처리 요소) 배열

        -   모든 PE는 SRAM에 네트워크 파티션을 저장하고 해당 부품과
            관련된 계산을 수행

        -   동적 입력 벡터 간격, 정적 중량 간격, 상대 지수화, 가중치
            공유 및 극히 적은가중치(4비트)을 활용

        -   각 PE는 원래 밀도 모델의 1.2M 가중치에 해당하는 압축 모델의
            131K 가중치를 가지며 초당 8억 개의 중량 계산을 수행 가능

        -   45nm CMOS 기술에서 EIE PE는 면적이 0.638mm2이고 800MHz에서
            9.16mW를 소산

        -   AlexNet의 FC 레이어는 총 40.8mm2를 소비하는 64PE에 적합,
            590mW의 전력 소산으로 41.88 × 10 프레임/초에서 작동

        -   CPU(Intel i7-5930k) GPU(GeForce TITAN X)와 모바일 GPU(Tegra
            K1)와 비교했을 때, EIE는 각각 189×, 13×, 307×가속성을
            달성하고, 2만4천×, 3,400×, 2,700x의 에너지를 절약

-   본 논문에서 나오는 내용

    -   Sparse와가중치를 공유하는 신경망을 위한 최초의 가속기를 제시

        -   압축된 네트워크에서 직접 작동 시 대규모 신경 네트워크 모델이
            온칩 SRAM에 적합하므로 외부D램의 액세스에 비해 120배 더 높은
            에너지 절약 효과

    -   EIE는 연산을 절약하기 위해 활성화의 동적 간격성을 이용하는
        최초의 가속기

        -   일반적인 심층 학습 애플리케이션에서 0인 활성화의 70%에 대한
            가중치 기준과 arithmetic을 피함으로써 65.16%의 에너지를 절약

    -   분산형스토리지와분산형 컴퓨팅의 방법을 설명

        -   여러 PE에 걸쳐 Sparasized 계층을 병렬화하여 부하 균형과
            확장성 달성

    -   객체 감지를 위한 CNN, 자연 언어 처리 및 이미지 캡션을 위한 LSTM
        등 다양한 심층 학습 모델에 대해 EIE를 평가

        -   EIE를 CPU, GPU, FPGA 그리고 다른 ASIC 가속기와 비교

3.  Motivation

-   MxV Matrix(매트릭스벡터 곱셈)

    -   신경망과 심층학습 어플리케이션에서 기본적인 구성 블록

    -   convolutional 신경망(CNN)에서는 FC 레이어가 M×V로 구현, 연결부의
        96% 이상이 FC 레이어에 있음

    -   객체 감지 알고리즘에서 FC 레이어는 모든 제안 영역에서 여러 번
        실행되어야 하며 최대 38%의 계산 시간이 소요

    -   반복 신경망(RNN)에서는 각 단계에서 새로운 입력과 숨겨진 상태에
        대해 M×V 연산을 수행, 새로운 숨겨진 상태와 출력을 생성

    -   STM(Long-Short-Term-Memory)은 보다 복잡한 숨겨진 단위 연산을
        제공하는 RNN 셀의 널리 사용되는 구조

    -   각 LSTM 셀은 8개의 M×V 작동으로 분해 가능, 각각 2개씩인 입력
        게이트, forget 게이트, 출력 게이트 및 임시 메모리 셀을 포함

    -   LSTM를 포함한 RNN은 이미지 캡션, 음성 인식 및 자연어 처리에 널리
        사용

-   메모리 액세스의 병목 현상

    -   입력 매트릭스는 재사용할 수 없으므로 모든 작업에 메모리 액세스가
        필요

    -   CPU와 GPU에서 이 문제는 보통 여러 벡터를 매트릭스로 결합하여
        파라미터를 재사용함으로써 해결

        -   그러나 이러한 전략은 자동 차량의 보행자 감지와 같이 대기
            시간에 민감한 실시간 애플리케이션에 적합하지 않음

        -   따라서 일괄처리 지연비용 없이 대규모 신경망을 실행하는
            효율적인 방법을 만드는 것이 바람직

    -   신경망을 압축

        -   압축은 총 작업 수를 감소시키지만, 압축에 의한 불규칙한
            패턴은 CPU와 GPU에서 효과적인 가속을 방해

    -   압축 네트워크는 이전의 가속기에서도 효율적이지 않음

        -   이전의 SPMV 가속기는 정적 중량 간격만 이용할 수 있음

            -   동적 활성화 빈도를 이용할 수 없음

        -   이전의 DNN 가속기는 어떤 형태의 간격도 이용할 수 없으며
            작동하기 전에 네트워크를 밀도 있는 형태로 확장해야 함

        -   둘 다 가중치 분담을 이용할 수 없음

4.  DNN Compression and Parallelization

    A.  Computation

        -   DNN의 FC 레이어가 계산을 수행함

![](media/image1.png){width="5.663888888888889in"
height="0.2986111111111111in"}

-   여기서 a는 입력 활성화 벡터, b는 출력 활성화 벡터, v는 편향, W는
    가중치 매트릭스, f는 비선형 함수

    -   일반적으로 CNN과 일부 RNN에서는 ReLU를 추가하여 W와 결합하는
        경우가 있으므로 편향을 무시

-   VGG-16의 FC7이나 AlexNet과 같은 일반적인 FC 레이어의 경우, 활성화
    벡터는 길이가 4K이고, 중량 매트릭스는 4K × 4K(16M 가중치)

    -   가중치는 단일 정밀 부동 소수점 숫자로 표시되므로 그러한 층은
        64MB의 저장 공간을 필요로 함

-   방정식(1)의 출력 활성화는 다음과 같이 요소별로 계산

![](media/image2.png){width="5.663888888888889in"
height="0.8659722222222223in"}

-   Deep Compression은 프루닝 및 가중치 분담을 조합하여 정확도 손실 없이
    DNN을 압축하는 방법을 설명

    -   프루닝은 벤치마크 계층의 경우 4% \~ 25%의 밀도를 가진 행렬 W를
        희박하게 만듬

    -   가중치 분량은 각 체중 Wij를 4비트 지수 Iij로 대체하여 16개의
        가능한 가중치 값의 공유 테이블 S로 함

-   Deep Compression 방식을 사용하면 방정식(2)의 활성화당 연산은 다음
    식과 같음

![개체이(가) 표시된 사진 자동 생성된
설명](media/image3.png){width="5.663888888888889in"
height="0.9326388888888889in"}

-   Xi는 Wij̸= 0, Y는 aj̸= 0, Iij는 Wij를 대체하는 공유 가중치에 대한
    지수, S는 공유 가중치 테이블

    -   여기서 Xi는 W의 정적 간격을 나타내고 Y는 a의 동적 간격을 나타냄

    -   세트 Xi는 주어진 모델에 대해 고정되어 있다. 세트 Y는
        입력마다다름

-   압축된 DNN을 가속하기 위해서는 가속식 (3)이 필요

    -   지수화 S \[Iij\]와 곱셈을 Wij와 aj가 모두 0이 아닌 열에 대해서만
        수행하여 매트릭스와 벡터의 간격을 모두 이용함

    -   동적으로 불규칙한 계산을 초래

    -   지수화 수행 자체는 4비트 Iij와 추가 부하(캐시 적중으로 거의
        확실시되는)를 추출하기 위한 비트 조작을 포함

B.  Representation

    -   활성화의 빈도를 이용하기 위해 인코딩된 희박 중량 행렬 W를 압축된
        희박 열(CSC) 형식의 변형으로 저장

    -   매트릭스 W의 각 열 Wj에 대해 0이 아닌 가중치를 포함하는 벡터 v와
        v의 해당 항목 앞에 0의 수를 인코딩하는 동일한 길이의 벡터 z를
        저장

        -   v와 z의 각 항목은 4비트 값으로 표현

        -   0이 아닌 항목 앞에 15개 이상의 0이 나타나면 벡터 v에 0을
            추가

        -   예를 들어, 다음 열을 인코딩한다.

            -   \[0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3\]

        -   v = \[1,2,0,3\], z = \[2,0,15,2\]. 모든 열의 v와 z는 포인터
            벡터 p가 각 열에 대한 벡터의 시작을 가리키도록 하여 하나의
            큰 쌍의 배열에 저장

        -   마지막 벡터 요소보다 한 포인트 높은 p 지점의 최종 항목은 j
            열(패딩된 0 포함)에 있는 0이 아닌 것의 수가 pj+1 - pj 에
            의해 주어지도록 함

    -   희소 행렬을 CSC 형식으로 열별로 저장하면 활성화 빈도를 쉽게 이용
        가능

        -   각각의 0이 아닌 활성화를 해당 열에 있는 0이 아닌 모든 원소로
            곱하면 됨

C.  Parallelizing Compressed DNN

    -   매트릭스 W의 행을 다중 처리 요소(PE) 위에 교차, 매트릭스-벡터
        연산을 병렬화

        -   N PEs를 사용하는 경우 PE k는 모든 행 Wi, 출력 활성화 bi 및
            입력 활성화 ai를 유지하며 i(model N) = k

        -   PEk에서 열 Wj 부분은 섹션 III-B에서 설명한 CSC 형식으로
            저장되지만, 0 카운트는 이 PE에서 열의 하위 집합에서 0만 참조

        -   각 PE에는 희소 행렬의 일부를 인코딩하는 자체 v, x 및 p
            배열이있음

![](media/image4.png){width="4.111805555555556in"
height="3.1638888888888888in"}

-   그림 2는 입력 활성화 벡터 a(길이 8)에 16×8 무게 매트릭스 W를 곱하여
    N = 4 PE에 출력 활성화 벡터 b(길이 16)를 산출하는 예시

    -   a, b, W의 요소는 PE 할당으로 색상으로 구분

    -   각 PE는 4줄의 W, 2개의 a의 요소, 4개의 b의 요소를 소유

-   우리는 다음 0이 아닌 값 aj를 찾기 위해 벡터 a를 스캔하고 모든 PE에
    지수 j와 함께 aj를broadcast 하여 희소 행렬 × 희소 벡터 작동을 수행

    -   각 PE는 열 Wj의 부분에 있는 0이 아닌 요소로 aj를 곱하여 출력
        활성화 벡터 b의 각 요소에 대한 축전지의 부분 합계를 축적

    -   CSC 표현에서 이러한 0이 아닌 중량은 연속적으로 저장되므로 각
        PE는 단순히 가중치를 적재하기 위해 위치 pj에서 pj+1 - 1까지 v
        어레이를 통과

    -   출력 축적기를 다루기 위해 각 중량 Wij에 해당하는 행 번호 i는 x
        배열 항목의 러닝 합계를 유지함으로써 생성

-   그림 2의 예에서 첫 번째 0이 아닌 것은 P E2의 a2

    -   값 a2와 그 열 인덱스 2는 모든 PE로broadcast

    -   그런 다음 각 PE는 2열의 각 부분에서 0이 아닌 모든 비율로 a2를 곱

    -   PE0은 2열에 a2를 W0,2와 W12,그리고 2열에 모두 0을 곱하기 때문에
        2열에 모든 0을 곱하지 않음

    -   PE2는 2와 W12를 곱하거나 해서 각 제품의 결과는 해당 행 축적기로
        요약

-   interleaved CSC 표현은 활성화 벡터 a의 동적 간격과 중량 행렬 W의
    정적 간격 모두를 용이하게 함

    -   입력 활성화 a의 0이 아닌 요소만 broadcast 함으로써 활성화 빈도를
        이용

    -   a에서 0에 해당하는 열은 완전히 생략

    -   interleaved CSC 표현은 각 PE가 각 열에 aj 로 곱하기 위해 0이
        아닌 값을 신속하게 찾을 수 있도록 함

        -   또한 이 조직은 입력 활성화의 방송을 제외한 모든 연산을 PE에
            로컬로 유지

```{=html}
<!-- -->
```
-   ![하늘, 벽, 시계이(가) 표시된 사진 자동 생성된
    설명](media/image5.png){width="5.850694444444445in"
    height="1.2236111111111112in"}

    -   현재 매트릭스의 interleaved CSC 표현은 위와 같음

    ```{=html}
    <!-- -->
    ```
    -   이 프로세스는 각 PE가 특정 열에 다른 수의 0이 아닌 값을 가질 수
        있기 때문에 load imbalance를 겪을 수 있음

5.  Hardware Implementation

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image6.png){width="6.261111111111111in"
height="1.5298611111111111in"}

-   위 그림은 EIE의 아키텍처

    -   CCU(Central Control Unit)는 각각 압축 네트워크의 한 조각을
        계산하는 일련의 PE를 제어

    -   CCU는 또한 분산 선도 비영점 감지 네트워크로부터 0이 아닌 입력
        활성화를 수신하고 이를 PE에 broadcast

-   EIE의 거의 모든 연산은 모든 PE에 방송되는 0이 아닌 입력 활성화의
    수집을 제외하고 PE에 local함

    -   그러나 대부분의 PE가 각 입력 활성화를 소비하는 데 많은 사이클이
        걸리기 때문에 활성화 수집 및 브로드캐스트의 타이밍은 중요하지
        않음

    A.  Activation Queue and Load Balancing

    -   입력 활성화 벡터 aj의 0이 아닌 요소와 해당 인덱스 j는 CCU에 의해
        각 PE의 활성화 대기열로브로드캐스트

        -   PE에 전체 대기열이 있는 경우 브로드캐스트가비활성화

        -   언제라도 각 PE는 대기열의 맨 앞에 있는 활성화를 처리

    -   활성화 대기열은 각 PE가 특정 열의 0이 아닌 수가 PE에서 PE까지
        다를 수 있기 때문에 발생할 수 있는 load imbalancing을 제거하기
        위해 작업 백로그를 작성

    -   섹션 VI에서는 활성화 대기열의 깊이에 대한 성능의 민감도를 측정

    B.  Pointer Read Unit

    -   활성화 대기열 맨 앞에 있는 항목의 인덱스 j는 v 및 x 열의 경우
        시작 및 끝 포인터 pj와 pj+1을 확인하는데 사용

    -   단일 포팅 SRAM 어레이를 사용하여 두 개의 포인터를 한 사이클로
        읽을 수 있도록 하기 위해 우리는 두 개의 SRAM 뱅크에 포인터를
        저장하고 주소의 LSB를 사용하여 bank 간에 선택

        -   pj와 pj+1은 항상 다른 bank에 있음

        -   EIE 포인터의 길이는 16비트

    C.  Sparse Matrix Read Unit

    -   스파스 매트릭스 읽기 장치

        -   포인터 pj와 pj+1을 사용하여 스파스 매트릭스 SRAM에서 이 PE
            컬럼 Ij의 0이 아닌 요소(있는 경우)를 read

        -   SRAM의 각 항목은 길이가 8비트이고 v의 4비트 요소 1개와 x의
            4비트 요소를 포함

    -   효율성을 위해 64비트 폭의 SRAM에 인코딩된 스파스 매트릭스 I의 PE
        슬라이스가 저장

        -   따라서 각 SRAM 판독값에 대해 8개의 입력을 가져오게 됨

        -   현재 포인터 p의 상위 13비트는 SRAM 행을 선택하고, 낮은
            3비트는 해당 행의 8개 항목 중 하나를 선택

        -   사이클마다 산술 단위에 단일(v, x) 입력이 제공

    D.  Arithmetic Unit

    -   산술 유닛은스파스 행렬 읽기 단위로부터 (v,x) 항목을 수신, 곱셈
        연산 bx = bx + v × aj를 수행

    -   인덱스 x는 축적기배열(대상 활성화 레지스터)를 인덱싱하는 데 사용

    -   v는 활성화 대기열 맨 앞에 있는 활성화 값으로 곱

        -   v는 4비트 인코딩 형태로 저장되기 때문에 먼저 테이블 조회를
            통해 16비트 고정 소수점 번호로 확장

    -   두 개의 인접 사이클에서 동일한 축적기를 선택한 경우, 부속품의
        출력을 입력으로 라우팅하기 위한 우회 경로가 제공

    E.  Activation Read/Write

    -   Activation Read/Write Unit에는 FC 레이어 계산의 단일 라운드 동안
        소스 및 대상 활성화 값을 각각 수용하는 두 개의 활성화 레지스터
        파일을 포함

        -   소스 및 대상 레지스터 파일은 다음 계층에 대한 역할을 교환

            -   따라서 다층 피드-포워드 계산을 지원하기 위해 추가적인
                데이터 전송은 필요하지 않음

    -   각 활성화 레지스터 파일에는 64개의 16비트 활성화

        -   64 PE에 걸쳐 4K 활성화 벡터를 수용하기에 충분

        -   보다 긴 활성화 벡터는 2KB 활성화 SRAM으로 수용 가능

        -   활성화 벡터의 길이가 4K보다 크면 M×V는 여러 배치로 완성, 각
            배치의 길이는 4K 이하

        -   모든 로컬 reduction은 레지스터 파일에서 이루어짐

        -   SRAM은 배치의 시작 부분에서만 읽고 마지막에 작성

    F.  Distributed Leading Non-Zero Detection

    -   입력 활동은 각 PE에 계층적으로 배포

        -   입력 벡터 간극을 활용하기 위해 선행 비영점 검출 논리를
            사용하여 첫 번째 0이 아닌 결과를 선택

        -   4개의 PE로 구성된 각 그룹은 입력 활성화에 대해 지역적으로
            0이 아닌 수의 검출 작업을 수행

        -   결과는 그림 4에 표시된 선행 비영점 감지 노드(LNZD 노드)로
            전송

        -   각 LNZD 노드는 네 명의 자식에서 다음 non-zero 활성화를 찾아
            이 결과를 쿼드 트리 위로 전송

            -   우리가 PE를 추가할 때 와이어 길이가 일정하게 유지되도록
                쿼드 트리가 배치

        -   루트 LNZD 노드에서 선택한 0이 아닌 수의 활성화가 H 트리 안에
            배치된 별도의 와이어를 통해 모든 PE로 다시 broadcast

    G.  Central Control Unit

    -   중앙 제어 장치(CCU)는 루트 LNZD 노드

    -   CPU와 같은 마스터와 통신하고, 제어 레지스터를 설정하여 모든 PE의
        상태를 감시

    -   I/O와 컴퓨팅의 두 가지 모드

        -   I/O 모드에서 모든 PE의 활성화와 가중치는 중앙 장치에 연결된
            DMA로 접근할 수 있는 동안 모든 PE는 공회전

        -   컴퓨팅 모드에서 CCU는 LNZD 쿼드트리로부터 0이 아닌 값을
            반복적으로 수집하고 이 값을 모든 PE에 브로드캐스트

            -   이 과정은 입력 길이가 초과될 때까지 계속

    -   포인터 어레이의 입력 길이와 시작 주소를 설정함으로써 EIE는 다른
        레이어를 실행하게 됨

6.  Evaluation Methodology

    A.  Simulator, RTL and Layout

    -   동기 회로의 RTL 동작을 모델링하기 위해 가속기에 대한 사용자 정의
        사이클 정확도 C++ 시뮬레이터를 구현

        -   각 하드웨어 모듈은 RTL의 조합 논리와 플립 플롭에 대응하여
            전파 및 업데이트라는 두 가지 추상적 방법을 구현하는 개체로
            추상화

        -   시뮬레이터는 디자인 공간 탐사에 사용

        -   RTL 검증을 위한 체커 역할도 함

    -   영역, 전력 및 임계 경로 지연을 측정하기 위해 Verilog에 EIE의
        RTL을 구현

        -   RTL은 사이클 정확도 시뮬레이터에 대해 검증

        -   그런 다음 최악의 경우 PVT corner를 가진 TSMC 45nm GP 표준 VT
            라이브러리 아래의 Synopsys Design Compiler(DC)를 사용하여
            EIE를 합성

        -   Synopsys IC 컴파일러(ICC)를 사용하여 PE를 배치하고 라우팅

        -   SRAM 영역과 에너지 번호를 얻기 위해 Cacti 사용

        -   RTL 시뮬레이션에서 SAIF(전환 활동 상호 교환 형식)로 덤프된
            게이트 레벨 넷리스트까지의 toggle rate에 주석을 달고,
            Prime-Time PX를 사용하여 전력을 추정

    B.  Comparison Baseline

    -   EIE를 CPU, GPU, 모바일 GPU의 세 가지 다른 기성 컴퓨팅 장치와
        비교

        i.  CPU

        -   NVIDIA Digits Deep Learning Dev Box에서 사용되어 온
            Haswell-E 클래스 프로세서인 Intel Core i-7 5930k CPU를 CPU
            기준으로 사용

        -   벤치마크

            -   원래 밀도 모델을 구현하기 위해 MKL CBLAS GEMV를 사용

            -   압축된 스파스 모델에는 MKL SPBLAS CSRMV를 사용

            -   CPU 소켓과 DRAM 전력은 인텔이 제공하는 pcm-power
                유틸리티에서 보이는 대로의 세팅

        ii. GPU

        -   NVIDIA GeForce GTX Titan X GPU를nvidia-smi 유틸리티를 사용

            -   심층 학습을 위한 첨단 GPU로 사용하여 전력을 보고

        -   벤치마크

            -   우리는 CuB를 사용

            -   원래의 밀도층을 구현하기 위한 LAS GEMV 사용

            -   압축된 스파스 레이어의 경우, 스파스 매트릭스를 CSR
                형식으로 저장

                -   GPU에서 스파스 매트릭스-벡터 곱셈에 최적화된
                    cuSPARSE CSRMV 커널을 사용

        iii. Mobile GPU

        -   192 CUDA 코어를 가진 NVIDIA Tegra K1을 모바일 GPU
            베이스라인으로 사용

            -   밀도가 높은 모델에는 cuBlas GEMV

            -   압축된 스파스 모델에는 cuSPARSE CSRMV를 사용

            -   Tegra K1에는 전력 소비량을 보고할 소프트웨어
                인터페이스가 없음

                -   전력계로 총 전력 소비량을 측정한 다음, AC 대 DC 변환
                    손실 15%, 주변 부품에 의한 85%의 규제 효율 및 15%의
                    전력 소비량을 가정하여 Tegra K1에 대한 AP+DRAM
                    전력을 보고

    C.  Benchmark

    -   압축되지 않은 DNN 모델과 압축된 DNN 모델이라는 두 가지 모델의
        성능을 비교

        -   압축되지 않은 DNN 모델은 카페 모델 Zoo,NeuralTalk 모델
            Zoo\[7\]에서 구함

        -   압축 DNN 모델은 다른 논문에 설명된 대로 생산

        -   벤치마크 네트워크는 AlexNet, VGGNet, NeuralTalk에서 얻은 총
            9개의 레이어가 있음

        -   우리는 하드웨어 설계의 정확성을 검증하기 위해 Image-Net
            데이터셋과 Caffe 심층 학습 프레임워크를 황금 모델로 사용

7.  Experimental Results

![녹색, 텍스트이(가) 표시된 사진 자동 생성된
설명](media/image7.png){width="5.261111111111111in"
height="3.0972222222222223in"}

-   위 그림은 EIE 처리 요소의 배치도(위치 및 라우트 이후)

![텍스트이(가) 표시된 사진 자동 생성된
설명](media/image8.png){width="5.261111111111111in"
height="4.014583333333333in"}

-   Power/Areabreakdown은 위 표 참고

-   우리는 코드북 조회 및 주소 축적(병렬), 출력 활성화 읽기 및 입력
    활성화 곱하기(병렬), 이동 및 추가, 출력 활성화 쓰기 등 4개의
    파이프라인 단계를 도입하여 임계 경로 지연을 1.15ns로 낮춤

    -   활성화 읽기 및 쓰기 액세스 로컬 레지스터와 활성화 바이패스
        기능을 사용하여 파이프라인 위험을 방지

    -   800MHz에서 실행되는 64개의 PE를 사용하면 102 GOP/s의 성능을 얻을
        수 있음

    -   10배 무게의 간격과 3배 활성화 빈도를 고려하면, 동일한
        애플리케이션 처리량을 가지려면 밀도 DNN 가속기 3TOP/s가 필요

-   각 EIE PE의 총 SRAM 용량(Spmat+Ptr+Act)은 162KB

    -   활성화 SRAM은 2KB 저장 용량

    -   Spmat SRAM은 128KB로 압축된 가중치와 지수를 저장

    -   각 가중치는 4비트, 각 지수는 4비트

    -   가중치와 지수는 8비트로 그룹화하여 함께 취급

    -   Spmat 접근 폭은 64비트에서 최적화

    -   Ptr SRAM은 포인터를 CSC 형식으로 저장하는 32KB

    -   정상 상태에서 Spmat SRAM과 Ptr SRAM은 모두 64/8 = 8
        사이클마다액세스

-   Area과 Power은 SRAM이 사용,비율은 각각 93%, 59%

    -   각 PE는 9.157mW를 소비하는 0.638mm2

    -   4 PE의 각 그룹에는 0이 아닌 검출에 대한 LNZD 장치가 필요

    -   64 PE에 총 21개의 LNZD 장치가 필요(16 + 4 + 1 = 21)

    -   합성 결과에 따르면 LNZD 1개 단위는 0.023mW, 면적은 189um2로 PE의
        0.3% 미만

    A.  Performance

    -   AlexNet, VGG- 16, Neural Talk에서 선정된 9개의 벤치마크에서
        EIE를 CPU, 데스크톱 GPU 및 모바일 GPU와 비교

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image9.png){width="6.261111111111111in"
height="1.238888888888889in"}

-   각 벤치마크에는 7개의 열이 있으며, 압축된 네트워크에 대한 EIE의 계산
    시간을 CPU/GPU/TK1과 비교

-   시간은 CPU로 정상화

-   EIE는 범용 하드웨어를 크게 능가하며, CPU, GPU, 모바일 GPU보다 각각
    189배, 13배, 307배 빠름

```{=html}
<!-- -->
```
-   EIE의 이론적 계산 시간은 워크로드 GOP를 최대 처리량으로 나누어 계산

    -   실제 계산 시간은 부하 불균형으로 인한 이론적 계산 시간보다 약
        10% 더 많음

    -   그림 6에서 CPU/GPU/TK1과의 비교는 실제 계산 시간을 사용하여 보고

![](media/image10.png){width="6.261111111111111in"
height="2.2465277777777777in"}

-   모든 벤치마크에 대한 CPU/GPU/TK1/EIE의 Wall Clock 시간은 표 IV 참고

```{=html}
<!-- -->
```
-   EIE는 실시간 추론이 필요한 대기 시간 중심의 애플리케이션을 목표로 함

    -   배치 조립은 상당한 지연 시간을 더하기 때문에, 우리는 그림 6과
        같이 CPU와 GPU로 성능과 에너지 효율을 벤치마킹할 때 배치 크기 =
        1인 경우를 고려

    -   비교로서 표 IV의 배치 크기 = 64에 대한 결과도 제공

    -   EIE는 대부분의 플랫폼을 능가하며, 배치 사례에서 데스크톱 GPU에
        필적

-   EIE가 동일한 애플리케이션 처리량(Frames/s)을 달성하는 데 필요한
    GOP/s는 EIE가 빈도를 이용하여 밀도가 높은 접근법에 의해 수행되는
    GOP/s의 97%를 제거하기 때문에 경쟁 접근법보다 훨씬 낮음

-   압축되지 않은 네트워크에서 3 TOP/s는 100 GOP/s만 필요

-   EIE의 처리량은 256PE 이상으로 확장 가능

    -   그러나 EIE 전용 logic이 없어도 CPU/GPU에 적용된 모델 압축
        자체만으로도 3배 속도의 속도를 낼 수 있음

B.  Energy

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image11.png){width="6.26875in"
height="1.2013888888888888in"}

-   그림 7에서는 다양한 벤치마크에서 M×V의 에너지 효율 비교를 보고

    -   각 벤치마크에는 7개의 열이 있으며, 압축 네트워크에서의 EIE의
        에너지 효율을 CPU/GPU/TK1과 비교

    -   에너지는 섹션 V에서 설명한 대로 계산 시간과 총 측정 전력을
        곱하여 얻음

-   EIE는 CPU, GPU, 모바일 GPU에 비해 평균 2만4000배, 400배, 2700배 적은
    에너지를 소비

    -   3단계의 규모 에너지 절약

        -   메모리 읽기당 필요한 에너지가 절약(SRAM over D램)

            -   압축 네트워크 모델을 사용하면 최첨단 신경 네트워크가
                온칩 SRAM에 들어갈 수 있어 D램에서 밀도가 높은 비압축
                모델을 가져오는 것에 비해 에너지 소비를 120배 줄일 수
                있음

        -   필요한 메모리 읽기 횟수가 줄어듬

            -   압축된 DNN 모델은 각 중량이 4비트 정량되는 가중치의
                10%를 가짐

        -   벡터 스패시리티를 활용하면 65.14%의 중복 연산 주기를 절약할
            수 있음

    -   이러한 요인을 120×10×8×3 곱하면 28, 800배의 이론적 에너지 절약

    -   우리의 실제 절약량은 지수 오버헤드 때문에 이 수보다 약 10배 더
        적음

    -   EIE는 Titan-X GPU와 Tegra K1 모바일 GPU가 사용하는 28nm 기술에
        비해 45nm 기술에서 구현되기 때문임

C.  Design Space Exploration

    i.  Queue depth

    -   활성화 FIFO 큐는 PE 간의 load imbalance를 다룸

![](media/image12.png){width="6.26875in" height="1.1118055555555555in"}

-   더 깊은 FIFO 큐는 생산자와 소비자를 더 잘 분리할 수 있지만, 그림 8의
    실험에서 보듯이 이득이 감소

-   64 PE를 사용하여 9개의 벤치마크에 걸쳐 FIFO 대기열 깊이를 1에서
    256까지 변경하고 부하 균형 효율성을 측정

-   이 효율은 1 - 버블 사이클(starvation으로 인한)을 총 계산 사이클로
    나눈 값으로 정의

-   FIFO 크기 = 1에서 전체 사이클의 약 절반은 유휴 상태이고 가속기는
    심각한 load imbalance을 겪음

-   FIFO 깊이가 증가하면 부하 불균형이 감소하지만 깊이가 8을 초과하여
    감소

    -   최적의 대기열 깊이로 8을 선택

```{=html}
<!-- -->
```
-   NT-We 벤치마크는 다른 기준에 비해 로드 밸런싱 효율성이 낮음

    -   600줄밖에 없음

    -   64 PE로 나누어진 후 11%의 간격(sparity)을 고려하면, 각 PE는 평균
        단일 입력을 얻는데, 이는 PE 간의 변화에 매우 취약하여 부하
        불균형을 초래

    -   작은 매트릭스는 32개 이하의 PE에서 보다 효율적으로 실행

ii. SRAM width

-   총 에너지를 최소화하기 위해스파스 행렬(Spmat)을 저장하기 위해 64비트
    인터페이스를 가진 SRAM을 선택

-   Wider SRAM 인터페이스는 총 SRAM 액세스 수를 줄이지만 SRAM 읽기당
    에너지 비용을 증가

-   실험적인 절충은 그림 9 참고

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image13.png){width="6.26875in"
height="1.4326388888888888in"}

-   SRAM 에너지는 45nm 이하에서 Cacti를 사용하여 모델링

-   SRAM 액세스 시간은 AlexNet 벤치마크의 사이클 정확 시뮬레이터에 의해
    측정

    -   오른쪽에 총 에너지가 표시되므로 SRAM 너비가 64비트일 때 최소 총
        접근 에너지가 달성

    -   SRAM 너비가 큰 경우 읽기 데이터가 낭비

    -   FC 레이어의 일반적인 활성화 요소 수는 4K이므로 64 PE와 10%
        밀도를 가정하면 PE의 각 열에는 평균 6.4 요소

        -   이는 8개의 요소를 제공하는 64비트 SRAM 인터페이스와 일치

        -   더 많은 요소를 가져오면 다음 열이 영점 활성화에 해당하면
            해당 요소는 낭비

iii. Arithmetic Precision

-   16비트 고정 소수점 연산을 사용

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image14.png){width="5.663888888888889in"
height="2.738888888888889in"}

-   그림 10과 같이 16비트 고정점 곱셈은 32비트 고정점보다 5배 적은
    에너지를 소비하고 32비트 부동점보다 6.2배 적은 에너지를 소비

    -   동시에 16비트 고정점 산술을 사용하면 예측 정확도의 0.5% 미만의
        손실이 발생

    -   32비트 부동점 산술을 사용하는 80.3%와 비교했을 때 79.8%

    -   8비트 고정점수로 정확도는 53%에 그쳐 사용 불가

    -   정확도는 AlexNet\[1\]을 사용하여 ImageNet 데이터 세트\[29\]에서
        측정, 에너지는 45nm 공정에서 합성된 RTL에서 얻음

8.  Discussion

    A.  Workload Partitioning

    -   첫 번째 접근법은 행렬 열을 PE에 분배하는 것

        -   각 PE는 출력 벡터 b의 부분 합계를 얻기 위해 W 열과 a의 해당
            요소 사이의 곱셈을 처리

        -   장점: 각 요소가 하나의 PE에만 연관되어 있다는 것

            -   즉 벡터 a에 대한 완전한 지역성을 제공

        -   단점:최종 결과를 얻기 위해 PE 간 감소 작업이 필요하다는 것

    -   두 번째 접근법(ours)은 행렬을 PE에 분배하는 것

        -   중앙 장치는 하나의 벡터 요소 aj를 모든 PE에 broadcast

        -   각 PE는 벡터 a와 함께 PE에 저장되어 있는 해당 W, Wj 행의
            내부 제품을 수행함으로써 다수의 출력 활성화를 연산

        -   장점: b의 각 요소가 하나의 PE에만 연관되어 있다는 것과 벡터
            b에 완전한 지역성을 제공

        -   단점:벡터가 모든 PE에 broadcast되어야 함

    -   세 번째 접근방식은 2D 방식으로 PE에 W 블록을 분배함으로써 이전의
        두 접근방식을 결합

        -   이 솔루션은 통신 지연 비용이 큰 분산 시스템에 대해 확장성이
            더 높음

            -   이렇게 하면 집합적 통신 사업 \"브로드캐스트\"와
                \"감소\"가 모두 이용되지만 규모가 작기 때문에 이
                해결책은 더 확장성이 있음

    -   대상 애플리케이션 등급의 특성과 빈도 패턴은 제약조건에 영향을
        미치며 따라서 분할과 저장 선택에도 영향을 미침

        -   W의 밀도는 ≈ 10%이며, a의 밀도는 ≈ 30%이며, 둘 다 무작위
            분포

        -   벡터 a는 정상 밀도 형식으로 저장되며 메모리에 0의 70%를
            포함하고 있는데, 입력에 따라 aj의 간격 패턴이 다름

        -   W와 a의 Sparse을 모두 활용하기를 원함

            -   첫 번째 해결책은 벡터 a도 희박하다는 점을 감안할 때 load
                imblance에 시달림

                -   각 PE는 칼럼을 담당

                -   PEj는 해당 요소 aj가 0일 경우 완전히 유휴 상태가 됨

                -   유휴 PE 외에도 이 솔루션은 PE 간 감소와 추가 수준의
                    동기화를 필요

            -   SPMV 엔진은 PE 수가 제한적이기 때문에, 걱정할 확장성
                문제는 없을 것

                -   그렇지만하이브리드 솔루션은 동일한 열을 공유하는
                    여러 PE가 유휴 상태로 남아 있을 수 있기 때문에
                    고유한 복잡성과 여전히 가능한 load imblance에 시달릴
                    것

        -   벡터의 30% 밀도를 고려한 두 번째 분배 계획에 근거하여 우리의
            솔루션을 구축

            -   해결책은 a에서 0이 아닌 수를 순서대로 조회하여 계산을
                수행하는 것을 목표

                -   각 PE는 순서에 따라 a의 0이 아닌 모든 요소를 얻고,
                    aj , Wj로 곱해야 하는 일치 요소를 찾아 수행

                -   이렇게 하려면 Matrix W를 CSC 형식으로 저장해야
                    하므로 PE는 aj에 의한 W의 j번째 열에 있는 모든
                    요소를 곱할 수 있음

    B.  Scalability

    -   매트릭스가 커짐에 따라 PE를 더 추가함으로써 시스템을 확장 가능

        -   각 PE에는 중복 없이 매트릭스의 개별 행을 저장하는 로컬
            SRAM이 있으므로 SRAM을 효율적으로 활용

    -   배선 지연은 PE 수의 제곱근에 따라 증가하지만, 이 아키텍쳐에서는
        문제 없음

        -   EIE는 전체 컬럼의 연산에 대해 하나의 broadcast만 요구하므로,
            많은 사이클이 소요

        -   결과적으로, broadcast은 결정적인 경로에 있지 않으며 FIFO가
            생산자와 소비자를 분리하기 때문에 파이프라인으로 연결될 수
            있음

![필기구, 문구이(가) 표시된 사진 자동 생성된
설명](media/image15.png){width="6.26875in" height="1.19375in"}

-   그림 11은 EIE가 NT-We를 제외한 모든 벤치마크에서 양호한 확장성을
    달성했음을알 수 있음

    -   NT-We는 매우 작음(4096 × 600)

        -   크기가 600이고 간격이 10%에서 64 이상인 PE로 나누면 심각한
            부하 불균형이 발생

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image16.png){width="6.26875in"
height="1.2013888888888888in"}

-   그림 12는 PE가 다른 패딩 0의 수를 나타냄

    -   패딩 제로(padding zero)는 스파스 행렬에서 연속적으로 0이 아닌 두
        요소 사이의 점프가 4비트가 인코딩할 수 있는 최대 수인 16보다 클
        때 발생

    -   패딩 제로(pading zero)는 0이 아닌 것으로 간주되어 연산 낭비로
        이어짐

    -   더 많은 PE를 사용하면 매트릭스 분할로 인해 0이 아닌 요소 사이의
        거리가 작아지고 최대 거리 16을 인코딩하는 4비트로 충분할 것

![](media/image17.png){width="6.26875in" height="1.2013888888888888in"}

-   그림 13은 8과 동일한 FIFO 깊이로 측정한 다양한 PE 개수의 load
    balancing

    -   PE가 많아지면 load balace가 나빠지지만, 0 패딩 오버헤드가
        감소하여 대부분의 벤치마크의 효율이 일정하게 유지

    -   확장성 결과는 그림 11

C.  Flexibility

-   EIE는 큰 신경망을 위해 설계

    -   대부분의 레이어의 무게와 입력/출력은 EIE의 스토리지에 쉽게 맞을
        수 있음

    -   입력/출력 크기가 매우 큰 것에도(예를 들어 VGG-16의 FC6 레이어는
        입력 크기가 25088) EIE는 여전히 64PE로 이를 실행 가능

-   EIE는 희박한 신경 네트워크 가속이나 SPMV와 관련된 다른 작업에 대한
    범용 프로세서 지원 가능

    -   신경 네트워크 구조 중 하나는 특정 제어 시퀀스로 분해되어 EIE의
        레지스터에 제어 시퀀스를 작성함으로써 네트워크를 실행

-   EIE는 채널의 감소를 M × V로 바꾸어 1x1 및 3x3 Winograd convolution을
    지원할 수 있는 잠재력을 가지고 있음

    -   Winograd convolution은 pure convolution보다 2.25배의 곱셈을 절약

    -   각 Winograd 패치의 경우 16 M × V를 EIE에 사용 가능

9.  Conclusion

-   FC 심층 신경망의 계층은 매트릭스-벡터 곱셈을 수행

    -   재사용을 개선하기 위해 일괄 처리를 사용할 수 없는 실시간
        네트워크의 경우, 이러한 계층은 메모리가 제한

    -   이러한 계층의 효율성을 개선하려면 매개변수를 가져오는 데 필요한
        에너지를 줄여야 함

-   이 논문은 압축된 심층 신경망에서 작동하기 위해 에너지 효율적인 엔진
    옵티마이징인 EIE를 제시

-   EIE는 활성화와 가중치 모두에서 Sparse을 활용하고 가중치 분담과
    정량화를 이용하여 일반적인 FC 레이어를 계산하는 데 필요한 에너지를
    GPU에 비해 3,400배 감소

-   에너지 절약에 대한 4가지 주요 요인

    -   파라미터의 수는 10배 정도 자름

    -   가중치 분배는 가중치를 4 bi로 줄임

        -   그러면 더 작은 모델은 DRAM이 아닌 SRAM에서 가져와 120배의
            에너지 이점을 얻을 수 있음

    -   그리고 활성화 벡터 또한 희박하기 때문에 최종 3배의 절약을 위해
        매트릭스 컬럼의 30%만 가져옴

-   EIE PE는 0.64mm2 면적의 1.6 GOPS를 실행하고 9mW만 소산할 수 있음

    -   64 PE는 1.88×104 프레임/초에서 알렉스넷의 FC 레이어를 처리할 수
        있음

    -   이 아키텍처는 에너지와 성능의 거의 선형적 확장으로 하나의 PE에서
        256 PE 이상으로 확장 가능

    -   FCLayer 벤치마크 9개에서 EIE의 성적

        -   CPU, GPU, 모바일 GPU를 189×, 13×, 307×로 능가

        -   각각 CPU, GPU, 모바일 GPU보다 2만4000×, 3,400×, 2, 700배
            적은 에너지를 소비
