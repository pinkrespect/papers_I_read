\[DL\] A Survey of FPGA-Based Neural Network Inference Accelerator

1.  Abstract

2.  Introduction

3.  Preliminary

    1.  Neural Network

    2.  FPGA-based Accelerator

4.  Design Methodology and Criteria

5.  Hardware Oriented Model Compression

    1.  Data Quantization

        1.  Linear Quantization

        2.  Non-Linear Quantization

        3.  Comparison

    2.  Weight Reduction

6.  Hardware Design: Efficient Architecture

    1.  Computation Unit Designs

        1.  Low Bit-width Computation Unit

        2.  Fast Convolution Method

        3.  Frequency Optimization Methods

    2.  Loop Unrolling Strategies

        1.  Choosing Unroll Parameters

        2.  Data Transfer and On-chip Memory Design

    3.  System Design

        1.  Roofline Model

        2.  Loop Tiling and Interchange

        3.  Cross-Layer Scheduling

        4.  Regularize Data Access Pattern

7.  Evaluation

    1.  Bit-width Reduction

    2.  Fast Convolution Algorithm

    3.  System Level Optimization

    4.  Comparison with GPU

8.  Technique Discussion

9.  Design Automation and Flexibility

    1.  Hardware Design Automation

    2.  Software Design Automation

    3.  Mixed Method

10. Conclusion

```{=html}
<!-- -->
```
1.  Abstract

-   신경망 추론의 높은 연산 및 저장 복잡성은 적용을 어렵게 하는 요소 중
    하나

    -   CPU 플랫폼은 충분한 연산 용량을 제공하기 어려움

    -   GPU 플랫폼은 높은 연산 용량과 개발 프레임워크를 사용하기 쉽기
        때문에 신경망 프로세스의 첫 번째 선택

-   FPGA 기반 신경망 inference accelerator

    -   특별히 설계된 하드웨어로 속도와 에너지 효율 면에서 GPU의 다음
        솔루션

    -   FPGA 기반 Accelerator 설계가 소프트웨어 및 하드웨어 최적화
        기법과 함께 제안

-   이 논문에서는

    -   FPGA를 기반으로 한 신경망 추론 가속기에 관한 이전의 연구 결과,
        사용된 주요 기법을 정리

    -   FPGA 기반 신경망 inference accelerator 설계의 분석을 완료하기
        위해 소프트웨어에서 하드웨어로, 회로 레벨에서 시스템 레벨에
        이르는 조사를 실시, 향후 작업에 대한 지침

2.  Introduction

-   신경망(Neural Network, NN)

    -   영상, 영상, 음성 처리를 위해 Convolutional 신경망(CNN)과 같은
        다양한 네트워크 모델 제안

    -   CNN은 ImageNet 데이터 세트의 상위 5개 이미지 분류 정확도를
        2012년 73.8%에서 84.7%로 개선, 기능 추출 능력이 탁월하여 개체
        감지를 개선하는데 도움이 됨

    -   RNN은 음성 인식에 대한 최신 단어 오류율 달성

    -   NN은 광범위한 패턴 인식 문제에 대한 높은 적합

    -   

-   NN 모델의 단점

    -   계산과 스토리지 복잡성이 높음

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image1.png){width="6.26875in" height="1.4555555555555555in"}

-   표 1에서는 최신 CNN 모델의 ImageNet 데이터 셋에 운용 횟수, 파라미터
    수(더하기 또는 곱셈) 및 Top-1 정확도를 열거

-   CNN의 경우

    -   224 × 224 이미지 분류를 위한 가장 큰 CNN 모델은 최대 390억 개의
        부동 소수점 연산(FLOP)과 500MB 이상의 모델 매개변수를 필요로 함

    -   연산의 복잡성은 입력 이미지 크기에 비례하므로 해상도가 높은
        영상을 처리하려면 1000억 개 이상의 연산이 필요할 수 있음

    -   MobileNet 및 ShuffleNet와 같은 최신 연구는 고급 네트워크 구조로
        네트워크 크기를 줄이려고 하지만 Accuracy loss 있음

    -   NN 모델의 크기와 정확도의 균형은 오늘날에도 여전히 미해결

    -   큰 모델 크기가 NN의 적용을 방해할 때도 있음

        -   전력 제한이나 대기 시간 임계 시나리오

-   neural-network-based 적용을 위한 적절한 계산 플랫폼을 선택하는
    필수적

    -   일반적인 CPU

        -   초당 10-100G의 플롭을 수행

        -   전력 효율은 대개 1GOP/J 미만

        -   CPU는 클라우드 애플리케이션에서 고성능 요구 사항이나 모바일
            애플리케이션에서 낮은 전력 요구 사항을 충족하기 어려움

    -   GPU

        -   최대 10TOP/s의 최고 성능을 제공

        -   성능을 신경 쓰는 신경 네트워크 애플리케이션에 적합

        -   또한 Caffe과 Tensorflow와 같은 개발 프레임워크에서 사용하기
            쉬운 인터페이스를 제공

    -   FPGA

        -   에너지 효율적인 신경망 처리 가능

        -   높은 병렬 프로그래밍 구현

        -   신경망 계산의 특성을 활용하여 추가적인 Logic 제거 가능

        -   NN 모델이 모델의 정확성을 해치지 않으면서 하드웨어 친화적인
            방법으로 단순화될 수 있음

            -   FPGA는 CPU와 GPU에 비해 높은 에너지 효율을 달성 가능

-   FPGA 기반 Accelerator 설계의 당면 과제

    -   CPU 및 GPU보다 훨씬 적은 100-300MHz의 작동 주파수를 지원

    -   재구성을 위한 Logic Overhead 또한 전체 시스템 성능을 감소시킴

    -   대한 단순 디자인은 높은 성능과 에너지 효율을 달성하기 어려움

    -   NN을 구현하는 것은 CPU나 GPU에서보다 훨씬 어려움

    -   CPU 및 GPU용 Caffe와 Tensorflow와 같은 개발 프레임워크가 없음

-   연구에서 제안된 기법을 다음과 같이 정리

    -   에너지 효율 설계의 방법론을 분석하기 위해 먼저 간단한 FPGA 기반
        신경 네트워크 Accelerator 성능 모델을 제공

    -   고성능 및 에너지 효율적인 신경망 Accelerator 설계를 위한 최신
        기술 조사

        -   소프트웨어와 하드웨어 레벨의 기술을 소개, 효과를 추정

    -   첨단 신경망 Accelerator 설계를 비교하여 도입된 기법 평가

    -   현재 GPU보다 최소 10배 이상 에너지 효율이 높은 FPGA 기반
        Accelerator 설계의 달성 가능한 성능 추정

    -   FPGA 기반 신경망 Accelerator의 최신 자동 설계 방법을 조사

3.  Preliminary

    1.  Neural Network

![텍스트, 지도이(가) 표시된 사진 자동 생성된
설명](media/image2.png){width="6.261111111111111in"
height="3.2090277777777776in"}

-   신경망의 기본 기능 소개

    -   숙련된 모델을 사용한 NN의 추론에만 초점, NN의 훈련 과정은 이
        논문에서 논의되지 않음

    -   신경망 모델은 그림 1(a)에 표시된 directed 그래프로 표현 가능

    -   그래프의 각 정점

        -   이전 계층 또는 입력의 데이터에 대한 작업을 수행하는 계층

        -   다음 계층 또는 출력에 대한 결과를 생성하는 계층

    -   각 층의 매개변수를 가중치

    -   각 층의 입력/출력을 본 논문을 통한 활성화로 간주

-   Convolution(CONV) 레이어와 Fully-Connected 레이어는 NN 모델에서 두
    가지 일반적인 유형의 레이어

    -   두 계층의 기능은 그림 1(b)에 표시

    -   CONV 레이어는 입력 피쳐 맵 $F_{\text{in}}$에서 2D Convolution을
        수행, 결과를 추가하여 출력 피쳐 맵 $F_{\text{out}}$으로 출력

    -   FC 레이어는 입력으로 피쳐 벡터를 받아 매트릭스-벡터 곱셈을 수행

-   그림 1(3)에서 VGG-11 모델에서 가중치와 오퍼레이션의 분포를 볼 수
    있음

    -   CONV와 FC 레이어는 함께 네트워크 가중치와 운영의 99% 이상을 기여

        -   대부분의 CNN 모델과 유사

        -   RNN 모델은 대개 CONV 레이어가 없고 FC 레이어만 대부분의
            계산과 저장에 기여

    -   그래서 대부분의 신경 네트워크 Accelerator 시스템은 이 두 종류의
        층에 초점

2.  FPGA-based Accelerator

-   FPGA-based accelerator의 장점

    -   FPGA는 개발자들이 목표 알고리즘에 따라 하드웨어에 필요한 논리만
        구현 가능

    -   일반 하드웨어 플랫폼에서 중복성을 제거, FPGA는 더 높은 효율성을
        달성 가능

    -   애플리케이션별 통합 회로(ASIC) 기반 솔루션은 훨씬 더 높은
        효율성을 달성하지만, 훨씬 더 긴 개발 주기와 더 높은 비용을
        필요로 함

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image3.png){width="6.261111111111111in"
height="2.6118055555555557in"}

-   FPGA 기반 신경망 가속기

    -   전형적인 구조는 그림 2(a)

    -   이 시스템은 보통 CPU 호스트와 FPGA 부품으로 구성

    -   순수 FPGA 칩은 보통 PCIe 연결을 통해 호스트 PC/서버와 함께 작동

        -   SoC 플랫폼(Xilinx Zynq Series와 같은)과 Intel HARPv2
            플랫폼은 호스트와 FPGA를 동일한 칩 또는 패키지에 통합

    -   호스트와 FPGA 모두 자신의 외부 메모리로 작업할 수 있고, 연결을
        통해 서로의 메모리에 접속 가능

    -   대부분의 설계는 FPGA 부분에 NN 액셀러레이터를 구현하고, 호스트
        상의 소프트웨어로 가속기를 제어

-   일반적인 FPGA 칩

    -   레지스터와 SRAM(Static Random- Access Memory)과 같은 대형 온-칩
        스토리지 유닛으로 구성

    -   그림 2(b)와 같이 NN 모델에 비해 작음

    -   일반 모델은 100-1000MB의 파라미터를 구현하지만, 가장 큰 FPGA
        칩은 \"50MB의 온-칩 SRAM\"을 구현

    -   이 차이는 DDR SDRAM과 같은 외부 메모리가 필요

    -   DDR의 대역폭과 전력 소비량은 시스템 성능을 제한

-   FPGA의 연산능력

    -   공통 FPGA는 수백에서 수천 개의 DSP 장치를 구현

    -   각각 18 × 27 또는 18 × 19를 계산 가능

    -   최대 FPGA에서 최대 10T FLOP/s(floating point operations per
        second) 달성

    -   Xilinx XC7Z020과 같은 저가형 FPGA의 경우 이 숫자는 20G FLOP/s로
        줄어들어 모바일 플랫폼의 애플리케이션에 대한 실시간 비디오
        처리를 지원 어려움

4.  Design Methodology and Criteria

-   설계 방법론의 개요

    -   일반적으로 신경망 추론 가속기의 설계 대상은 고속(고속 처리량과
        낮은 대기 시간)과 높은 에너지 효율의 두 가지 측면을 포함

    -   이 절에 사용된 기호

![텍스트이(가) 표시된 사진 자동 생성된
설명](media/image4.png){width="6.261111111111111in"
height="5.947916666666667in"}

-   속도

    -   NN 가속기의 처리량은 방정식 1로 표현 가능

![](media/image5.png){width="6.261111111111111in"
height="0.42569444444444443in"}

-   특정 FPGA 칩의 온-칩 자원은 제한

-   Peak 성능 증가 시키기

    -   각 계산 단위의 크기를 줄임으로써 연산 단위 P의 수를 증가시킬 수
        있음

    -   연산 단위의 크기를 줄이는 것은 데이터 정밀도를 희생시킴으로써
        달성될 수 있음

        -   이것은 모델 정확도를 해칠 수 있고 하드웨어-소프트웨어 공동
            설계가 필요

        -   반면에 작업 빈도를 높이는 것은 순수한 하드웨어 설계 작업

        -   소프트웨어 모델과 하드웨어에 대한 해당 기법은 섹션 4와 5에
            각각 소개

    -   High Utilization Ratio인 η은 합리적인 병렬 구축과 효율적인
        메모리 시스템으로 보장

        -   대상 모델의 속성, 즉 데이터 액세스 패턴 또는 데이터 계산
            비율도 하드웨어를 런타임에 완전히 활용할 수 있는지 여부에
            영향

        -   High Utilization Ratio 증가를 목표로 한 이전 작업의 대부분은
            하드웨어 쪽에 초점을 맞춤

    -   대부분의 FPGA 기반 NN 가속기는 서로 다른 입력을 하나씩 계산

    -   일부 설계는 서로 다른 입력을 병렬로 처리

    -   그래서 가속기의 대기 시간은 방정식 2로 표현

![](media/image6.png){width="6.261111111111111in"
height="0.42569444444444443in"}

-   공통적인 동시 설계는 레이어 파이프라인과 배치 처리를 포함

    -   이는 일반적으로 Loop Unrolling과 함께 고려되며 섹션 5.2에 소개

    -   이 논문에서는 처리량을 최적화하는 데 중점

        -   다른 NN 모델에서 서로 다른 가속기를 평가할 수 있기 때문에,
            속도의 공통 기준은 다른 네트워크 모델의 영향을 어느 정도
            제거하는 $\text{OPS}_{\text{act}}$ 사용

```{=html}
<!-- -->
```
-   에너지 효율.

    -   에너지 효율($\text{Eff}$)은 컴퓨터 시스템의 또 다른 중요한 기준

    -   신경망 추론 가속기의 경우 에너지 효율은 방정식 3으로 정의

![](media/image7.png){width="6.261111111111111in"
height="0.42569444444444443in"}

-   처리량과 마찬가지로, 작업량 W의 차이를 없애기 위해 추론 횟수(Number
    of inference)보다 작업 수(Number of operation)를 계산

-   대상 네트워크의 작업량이 고정되어 있는 경우, 신경망 가속기의 에너지
    효율을 높인다는 것은 각 입력을 처리하는 총 에너지
    비용($E_{\text{total}}$)을 절감하는 것을 의미

-   총 에너지 비용은 주로 계산과 메모리 접근의 두 부분에서 발생하는데,
    이 두 부분은 방정식 4로 표현

![](media/image8.png){width="6.261111111111111in" height="0.26875in"}

-   방정식 4의 첫 번째 항목은 계산을 위한 동적 에너지 비용

    -   어떤 네트워크가 주어진다면, 작업량 W는 고정되어 있음

    -   연구자들은 처리량 최적화와 유사한 규칙을 따르는 $N_{op}$을
        줄이기 위해 $E_{op}$를 줄이거나 0으로 가중치를 더 정량화하여 NN
        모델을 최적화하는 데 주력

-   방정식 4의 두 번째와 세 번째 항목은 메모리 접근을 위한 동적 에너지
    비용

    -   FPGA 기반 NN 액셀러레이터는 보통 외부 D램과 연동

    -   우리는 메모리 액세스 에너지를 DRAM 부분과 SRAM 부분으로 분리

    -   $N_{x\_ access}$는 정량화, 간극화, 효율적인 온칩 메모리 시스템
        및 스케줄링 방법에 의해 줄일 수 있음

    -   따라서 이러한 방법들은 동적 기억 에너지를 줄이는 데 도움이 됨

    -   특정 FPGA 플랫폼을 사용할 경우 단위 에너지 엑스를 거의 줄일 수
        없음

-   네 번째 항목인 $E_{\text{static}}$은 시스템의 정적 에너지 비용을
    나타냄

    -   이 에너지 비용은 FPGA 칩과 설계 규모를 고려할 때 거의 개선될 수
        없음

    -   속도와 에너지의 분석을 통해 신경 네트워크 가속기는 NN 모델과
        하드웨어의 최적화를 모두 수반한다는 것을 알 수 있음

5.  Hardware Oriented Model Compression

-   NN 모델의 최적화

    -   NN 모델이 크면 대개 모델 정확도가 높아지는데, 모델의 정확도는
        하드웨어 속도나 에너지 비용과 트레이드 오프가 가능

    -   신경망 연구자들은 AlexNet, ResNet, SqueezeNet, MobileNet까지
        보다 효율적인 네트워크 모델을 설계

        -   최신 연구는 네트워크 구조를 검색하여 처리 지연 시간을 직접
            최적화하거나 런타임에 일부 계층을 건너뛰어 계산을 저장하려고
            시도

        -   네트워크들 간의 주요 차이점은 크기와 각 계층 간의 연결

            -   기본적인 운영은 동일

            -   하드웨어 설계에 거의 영향을 미치지 않음

            -   이 논문에서는 이러한 기술에 초점을 맞추지 않음

            -   설계자는 대상 네트워크를 최적화하기 위해 이러한 기법을
                사용하는 것을 고려해야 함

        -   NN 모델을 압축함으로써 절충

            -   각 활성화 또는 가중치에 사용되는 비트의 수를 줄이려고
                노력

                -   계산과 저장 복잡성을 줄이는데 도움

                -   하드웨어 설계는 이러한 NN 모델 압축 방법의 이점을
                    얻을 수 있음

                -   이러한 하드웨어 지향 네트워크 모델 압축 방법을 조사

    1.  Data Quantization

    -   모델 압축에 가장 일반적으로 사용되는 방법 - 가중치와 활성화
        함수를 정량화

        -   일반적으로 공통 개발 프레임워크에서 부동 소수점 데이터로
            표현

        -   최근의 연구는 이 표현을 저비트 고정 소수점 데이터 또는
            훈련된 값의 작은 집합으로 대체

        -   각 활성화 함수/가중치에 대해 더 적은 비트를 사용하는 것은
            신경 네트워크 처리 시스템의 대역폭과 저장 요건을 줄이는 데
            도움

        -   간단한 표현을 사용하면 각 작업에 대한 하드웨어 비용을 줄일
            수 있음

        -   선형 정량화와 비선형 정량화의 두 가지 종류의 정량화 방법을
            설명

        1.  *Linear Quantization*

-   선형 정량화는 각 가중치와 활성화에 대해 가장 가까운 고정 소수점
    표현을 찾음

-   문제는 부동 소수점 데이터의 동적 범위가 고정 소수점 데이터의 동적
    범위를 훨씬 초과한다는 것

-   대부분의 가중치와 활성화 함수는 오버 플로우나 언더 플로우로 인해
    문제가 생김

-   Chiu et al.은 단일 층에서 가중치와 활성화의 동적 범위가 훨씬 더
    제한되어 있고 서로 다른 층에 걸쳐 다르다는 것을 발견

    -   서로 다른 계층의 가중치와 활성화에 서로 다른 fractional 비트
        너비를 할당

    -   데이터 집합의 부분 비트 width(즉, 레이어의 활성화 또는 가중치)을
        결정하기 위해 데이터 분포를 먼저 분석

    -   가능한 부분 비트 width 집합을 후보 솔루션으로 선택

    -   그런 다음 교육 데이터 세트에서 최고의 모델 성능을 갖춘 솔루션을
        선택

    -   네트워크의 최적화 솔루션은 exponential design space
        exploration를 피하기 위해 계층별로 선택

    -   Wang et al.은 첫 번째와 마지막 계층에만 큰 비트 너비를 사용하고
        중간 계층을 3차 또는 2진수로 정량화 하려고 함

        -   높은 정확도를 유지하기 위해 네트워크 크기를 증가시켜야 함

        -   그럼에도 불구하고 하드웨어 성능 향상

        -   Guo et al.은 모든 레이어의 부분 비트 폭이 고정된 후에 모델을
            미세 조정

        -   부분 비트 너비를 선택하는 방법

            -   스케일링 팩터 2로 데이터를 스케일링하는 것과 같음

            -   Li et al.은 각 레이어에 대해 훈련된 매개변수 $w^{l}$로
                가중치를 조정하고 $w^{l}$, 0 및 $- w^{l}$를 나타내는
                2비트 데이터로 가중치를 정량화

-   이 논문에서 활성화 함수는 정량화 되지 않았음

    -   네트워크는 여전히 32비트 부동 소수점 연산을 구현

        -   Zhou et al.은 1비트 \~ ±s로만 레이어의 가중치를 정량화

            -   여기서 s = E(\|$w^{l}$\|)는 이 레이어의 가중치의
                절대값의 기대치

            -   선형 정량화는 이 작업의 활성화 함수에도 적용

        1.  *Non-Linear Quantization*

-   비선형 정량화는 독립적으로 다른 이진 코드에 값을 할당

-   비선형 정량화 코드에서 해당 값으로 변환한 것은 따라서 Look-up
    Table이 됨

-   이러한 종류의 방법은 각 활성화 또는 가중치에 사용되는 비트 폭을 더욱
    줄이는 데 도움이 됨

-   Chen et al.은 미리 정의된 해시 함수에 의해 각각의 가중치를 Look-up
    Table의 항목에 할당하고 내부의 값을 트레이닝

-   Han et al. 은 트레이닝 된 모델의 가중치를 군집화, Look-up Table의
    값을 가중치에 할당

    -   각 Look-up table 값은 클러스터 센터로 설정되며 교육 데이터
        세트와 함께 세부 조정

    -   이 방법은 최신 CNN 모델의 가중치를 정확도 손실 없이 4비트로 압축
        가능

-   Zhu et al.은 계층의 모든 가중치를 $W^{n}$, 0 및 $W^{p}$의 세 가지
    값으로 정량화하는 ternary-정량화 네트워크를 제안

    -   정량화된 값과 가중치와 look-up table 사이의 대응은 모두 트레이닝

    -   이 방법은 ImageNet 데이터 세트에서 2% 미만의 정확도 손실

    -   가중치 비트 폭은 32비트에서 2비트로 줄어들어 모델 크기 압축이
        16배 정도 됨

        1.  *Comparison*

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image9.png){width="6.261111111111111in"
height="3.276388888888889in"}

-   그림 3은 모든 정량화 결과를 ImageNet 데이터 세트에서 테스트하고 해당
    기준 부동 소수점 모델과 비교한 절대 정확도 손실을 기록

-   선형 정량화의 경우 8비트는 정확도 손실을 보장하는 명확한 경계선

-   비트가 6개 이하일 경우 처음부터 각 가중치를 미세 조정하거나 트레이닝
    할 경우 정확도가 저하

-   1% 정확도 손실을 허용 범위 안에 두는 경우

    -   최소 8 × 8 구성의 선형 정량화

    -   위에 나열된 비선형 정량화를 사용 가능

    1.  Weight Reduction

-   모델 압축의 또 다른 방법

    -   가중치의 수를 줄이는 것

        -   낮은 순위표현으로 가중치 매트릭스를 근사화하는 것

            -   Qui et al.

                -   단일한 값 분해로 FC 레이어의 가중치 행렬 W를 압축

                -   m × n 중량 행렬 W는 두 행렬의 곱셈으로 대체

                -   충분히 작은 p의 경우, 총 가중치 수가 감소

                -   이 연구는 VGG 네트워크의 최대 FC 레이어를 0.04%의
                    분류 정확도로 원래 크기의 36%로 압축

            -   Zhang et al.

                -   convolution layer에도 이와 유사한 방법을 사용

                -   분해 최적화 프로세스에 비선형 레이어의 효과를 가져옴

                -   ImageNet 모델에서 4배 빠른 속도를 달성, 정확도의
                    손실은 0.9%

        -   프루닝(가지치기)

            -   가중치의 0을 직접 제거, 절대값이 작은 0을 제거

            -   프루닝의 어려운 점은 제로 웨이트의 비율과 모델 정확도
                사이의 절충

            -   한 가지 해결책은 트레이닝 중 가중치에 L1 Normalization를
                적용하는 lasso

                -   Liu et al.

                    -   AlexNet 모델에 sparse group-lasso 방법 적용

                    -   90%의 가중치는 훈련 후에 제거되며, 정확도 loss는
                        1% 내외

            -   훈련 중에 가중치가 0인 것으 잘라내는 것

                -   Han et al.

                    -   0이거나 작은 절대값을 가지는 네트워크의 가중치를
                        직접 제거

                    -   왼쪽 가중치를 훈련 데이터 세트에 미세 조정하여
                        정확도를 회복

                    -   AlexNet에 대한 실험 결과는 모델 정확도를
                        유지하면서 89%의 가중치를 제거할 수 있었음

        -   가중치 감소에 의한 하드웨어 증가는 압축 비율의 역수

        -   위의 결과에 따르면 가중치 가지치기에 따른 잠재적 속도 향상은
            최대 10배

6.  Hardware Design: Efficient Architecture

    1.  Computation Unit Designs

-   신경망 가속기의 최고 성능에 영향

-   FPGA 칩의 가용 자원은 제한되어 있고, computation unit의 설계가
    작을수록 연산 단위가 늘어나고 피크 성능이 높아짐

-   신중하게 설계된 computation unit array 또한 시스템의 작동 빈도를
    증가시켜 피크 성능을 향상시킬 수 있음

    1.  *Low Bit-width Computation Unit*

    -   연산을 위한 비트 폭의 수를 줄이는 것은 연산 단위의 크기를 줄이는
        직접적인 방법

    -   더 적은 비트 사용의 실현가능성은 섹션 4.1에 소개된 정량화 방법
        참고

        -   FPGA 설계는 32비트 부동 소수점 단위를 고정 소수점 단위로
            대체

    -   Podili et al

        -   32비트 고정 소수점 단위 구현

        -   16비트 고정 소수점 장치는 널리 채택되어 있음

    -   ESE

        -   12비트 고정점 무게와 16비트 고정점 뉴런 설계를 채택

    -   궈 외

        -   내장형 FPGA에서 8비트 유닛을 설계

        -   최근 연구도 극히 좁은 비트 폭 설계에 초점

    -   Prost-Boucle et al.

        -   3차 네트워크의 경우 1LUT로 2비트 곱셈을 구현

        -   BNN(Binarized Neural Network)의 FPGA 구현이 CPU 및 GPU보다
            우수하다는 것을 보여주는 실험.

        -   BNN은 정확도 손실이 크지만, 계산을 위해 1비트 데이터를
            사용하는 것의 이점을 탐구

    -   위의 설계는 선형 정량화를 위한 연산 단위에 초점

        -   비선형 정량화

            -   데이터를 완전한 정밀도로 변환하여 계산하는 것은 많은
                자원을 소모

                -   삼라흐 외

                    -   dot 제품 구현에 기초한 요인 계수를 제안

                    -   가중치의 가능한 값은 비선형 정량화에 대해 상당히
                        제한

                    -   제안된 연산 단위는 각각의 가능한 가중치 값에
                        대한 곱셈기를 축적

                    -   그 결과를 룩업 표에 있는 값의 가중치 합계로 계산

                    -   하나의 출력 뉴런에 필요한 곱셈은 조회 표의 값의
                        수와 같음

                    -   그 곱셈은 무작위 주소 축적으로 대체

    -   신경망의 과정을 통해 하나의 비트 너비를 사용

        -   Qui et al

            -   FC 레이어의 뉴런과 가중치는 정확도가 유지되는 동안 CONV
                레이어에 비해 더 적은 비트를 사용할 수 있다는 것을 발견

    -   서로 다른 비트 폭의 연산 단위의 크기를 표 3에 비교

![](media/image10.png){width="6.26875in" height="2.19375in"}

-   Xilinx FPGA의 로직 리소스로 승수와 가산기를 분리

-   Xilinx FPGA의 DSP 유닛을 이용한 곱하기, 가산기 기능

-   Altera FPGA의 DSP 유닛을 이용한 곱하기, 가산기 기능

    -   비바도 2018.1

        -   Xilinx XCKU060 FPGA, 알테라 아리아 10 GX1150 FPGA를 대상으로
            한 Quartus Prime 16.0을 대상으로 한 합성 결과

        -   순수 논리 모듈과 부동소수점은 IP코어를 사용하여 증식,
            추가모듈을 생성

        -   고정점 곱하기 및 추가 모듈은 Verilog에서 A \* B + C로 구현

        -   Vivado/Quartus에 의해 DSP에 자동으로 매핑

```{=html}
<!-- -->
```
-   우선 논리만 구현하는 것에 의한 계산 단위의 크기에 대한 개요를 제공

    -   32비트 부동 소수점 번호에서 8비트 고정 소수점 번호로 가중치와
        활성화를 압축함으로써 승수와 가산기는 각각 약 1/10과 1/50로 축소

    -   4비트 이하의 연산자를 사용하면 더 큰 이점을 얻을 수 있지만
        4.1절에 소개된 것과 같이 상당한 정확도 손실을 초래

-   최근의 FPGA는 많은 수의 DSP 유닛으로 구성되어 있으며, 각 유닛은
    단단한 승수, 사다리 및 축전지 코어를 구현

    -   NN 계산의 기본 패턴인 곱셈과 합도 이 설계에 들어맞음

    -   DSP 유닛으로 구현된 곱셈 및 덧셈 기능도 테스트

    -   DSP 아키텍처가 다르기 때문에 우리는 Xilinx 플랫폼과 Altera
        플랫폼 모두에서 테스트

    -   32비트 부동소수함수에 비해 비트 폭이 좁은 고정 소수점 함수는
        여전히 자원소비가 유리

    -   그러나 Altera FPGA의 경우 DSP 장치가 기본적으로 부동 소수점
        운영을 지원하기 때문에 이러한 이점은 명백하지 않음

-   16비트 또는 저비트 고정 소수점 데이터를 가진 고정 소수점 함수는
    Xilinx 또는 Altera FPGA의 1 DSP 장치에 잘 맞음

    -   우리가 계산의 측면에서 8 또는 4와 같이 더 좁은 비트 너비를
        사용한다면 정량화가 하드웨어에 거의 도움이 되지 않는다는 것을
        보여줌

    -   문제는 DSP 장치의 넓은 승수와 가산기가 이러한 경우에 충분히
        활용되지 않는다는 것

        -   Nguyen et al.

            -   하나의 넓은 비트폭 고정 소수점 곱셈기로 2개의 좁은
                비트폭 고정 소수점 곱셈을 구현하는 설계를 제안

            -   이 설계에서는 AB와 AC라는 두 개의 곱셈을 A(B \<\< k
                +C)의 형태로 실행

            -   k가 충분히 클 경우 AB와 AC의 비트는 곱셈 결과에서
                중복되지 않으며 직접 분리 가능

            -   \[45\]의 설계는 25 × 18 승수 1개를 갖는 2개의 8비트
                곱셈을 구현하며 여기서 k는 9

            -   유사한 방법을 다른 비트 폭과 DSP에도 적용 가능

    1.  *Fast Convolution Method*

-   CONV 레이어의 경우, 대체 알고리즘에 의해 컨볼루션 연산이 가속화

-   디지털 신호 처리에는 이산 푸리에 변환(DFT) 기반의 빠른 컨볼루션(Fast
    convolution)이 널리 채택

    -   장 외

        -   효율적인 CONV 계층 실행을 위한 2D DFT 기반 하드웨어 설계를
            제안

        -   K × K 필터가 연결된 F × F 필터의 경우, DFT는 공간 영역의
            (F - K + 1)2K2 곱셈을 주파수 영역의 F2 복합 곱셈으로 변환

        -   M 입력 채널과 N 출력 채널이 있는 CONV 레이어의 경우 주파수
            영역 곱셈의 MN 횟수와 (M + N ) 곱셈 DFT/IDFT가 필요

        -   콘볼루션 커널의 변환은 모두 한 번

            -   따라서 도메인 변환 프로세스는 CONV 계층에 대한 낮은 비용

            -   이 기법은 stride¿1 또는 1 × 1 convolution이 있는 CONV
                레이어에는 적용되지 않음

    -   딩 외

        -   중량 매트릭스에 블록-방향 원형 구속조건을 적용할 수 있음을
            제안

        -   이와 같이 FC 레이어의 매트릭스벡터 곱셈은 1D 코볼루션
            집합으로 변환되어 주파수 영역에서 가속

        -   K × K conolution kernes를 K × K 매트릭스로 처리하여 CONV
            레이어에 적용할 수 있으며 K나 스텝에 의해 제한되지 않음

-   주파수 영역 방법

    -   복잡한 숫자의 곱셈을 요구

    -   또 다른 종류의 Fast Convolution은 실제 숫자의 곱셈 만을
        포함한다.

    -   Winograd 알고리즘을 사용한 커널 K를 가진 2D 피쳐 맵 핀의
        콘볼루션은 방정식 5로 표현

![](media/image11.png){width="6.26875in" height="0.26875in"}

-   G, B, A는 커널과 피쳐 맵의 크기와만 관련된 변환 행렬

-   ⊙은 두 행렬의 원소 곱셈

-   3 × 3 커널로 구성된 4 × 4 피쳐 맵의 경우 변환 매트릭스는 다음과 같음

![개체, 시계이(가) 표시된 사진 자동 생성된
설명](media/image12.png){width="6.26875in"
height="0.8284722222222223in"}

-   변환 매트릭스 A, B, G와의 곱은 특별한 행렬 항목 때문에 소수의 이동과
    추가 만을 유도

-   이 경우 곱셈은 36개에서 16개로 줄어듬

-   가장 일반적으로 사용되는 Winograd 변환은 \[36, 70\]의 3 × 3 개
    콘볼루션에 대한 것

```{=html}
<!-- -->
```
-   빠른 콘볼루션으로 인한 이론적 성능 이득은 콘볼루션 크기에 따라
    달라짐

    -   온-칩 자원과 유연성에 대한 고려에 의해 제한되는 현재의 설계는 큰
        콘볼루션 크기를 배제

    -   FFT/Winograd

        -   합당한 커널 크기를 가진 Fast Convolution으로 최대 4배의
            이론적 성능 향상을 달성 가능

    -   Zhuge et al.

        -   다른 층에서 서로 다른 커널 크기를 맞추기 위해 FFT와 Winograd
            방법을 모두 설계에 사용하려고 시도

    1.  *Frequency Optimization Methods*

-   연산 장치의 프리퀀시, 주파수를 높이면 피크 성능도 향상

-   최신 FPGA는 700-900MHz DSP 이론적 피크 작동 주파수를 지원

    -   기존 설계는 대개 100-400MHz에서 작동

    -   작동 주파수는 온칩 SRAM과 DSP 장치 사이의 라우팅에 의해 제한

    -   DSP 장치와 주변 로직을 위해 서로 다른 작동 주파수를 사용

        -   각 DSP 장치에 대한 인접 슬라이스는 클럭 도메인을 분리하기
            위해 로컬 RAM으로 사용

        -   프로토타입 설계는 다른 속도 등급의 FPGA 칩에서 741MHz 및
            891MHz의 최고 DSP 작동 주파수를 달성

        -   Xilinx는 또한 이 기법으로 CHaiDN-v과 xfDN을 제안

        -   최대 700 MHz DSP 작동 주파수를 달성

        -   주파수가 300MHz 이내인 기존 설계와 비교하여 이 기술은 최소
            2배 이상의 피크 성능 향상

1.  Loop Unrolling Strategies

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image13.png){width="6.26875in" height="2.738888888888889in"}

1.  *Choosing Unroll Parameters*

-   루프의 실행을 병렬화 하기 위해, 우리는 루프를 풀고 하드웨어에서 일정
    횟수의 반복 과정을 병렬화

    -   하드웨어에서 병렬화된 반복 횟수를 unroll 파라미터

        -   부적절한 unroll 파라미터 선택은 심각한 하드웨어 활용을 초래
            가능

        -   루프의 트립 카운트가 M이고 병렬이 m이라고 가정

            -   하드웨어의 이용률은 m/M⌈M/m⌉에 의해 제한

            -   M을 m으로 나누지 않으면 이용률은 1보다 작음

            -   NN 레이어를 처리하기 위해, 총 이용률은 각 루프의
                이용률과 관련

    -   CNN 모델의 경우 루프 치수는 층마다 크게 다름

        -   ResNet와 같은 ImageNet 분류에 사용되는 일반적인 네트워크의
            경우

            -   채널 번호는 3에서 2048까지 다양

            -   형상 지도 크기는 224 × 224에서 7 × 7까지 다양

            -   콘볼루션 커널 크기는 7 × 7에서 1 × 1까지 다양

            -   활용도가 낮은 문제 이외에도 루프 언롤링은 데이터 경로와
                온칩 메모리 설계에도 영향

            -   루프 언롤링 전략은 신경망 가속기 설계의 핵심 기능이다.

-   언롤 파라미터를 선택하는 방법

    -   Zhang 외 연구진

        -   입력 채널과 출력 채널 루프를 풀고 설계 공간 탐사에 의해
            최적화된 언롤 파라미터를 선택하는 아이디어를 제안

        -   두 루프를 따라 인접 반복 사이에 입력 데이터가 교차 의존하지
            않음

        -   따라서 온-칩 버퍼에서 연산 장치로 데이터를 라우팅하는
            멀티플렉서는 필요하지 않음

        -   병렬은 7 × 64 = 448개의 승수로 제한

        -   더 큰 병렬의 경우, 이 해결책은 활용도가 낮은 문제를 겪기
            쉬움

    -   Ma 외

        -   형상 지도 루프의 병렬화를 허용함으로써 설계 공간을 더욱 확장

        -   병렬은 1 × 16 × 14 × 14 = 3136 승수에 도달

        -   시프트 레지스터 구조는 피쳐 맵 픽셀을 계산 단위로 라우팅하는
            데 사용

-   Motamedi et al.

    -   AlexNet에서 unrolling 커널을 사용

    -   11 × 11 및 5 × 5 커널에 대해 3 × 3을 unrolling 하더라도 전체
        시스템 성능은 여전히 convolution layer에 대한 피크 성능의
        97.4%에 도달

    -   VGG\[56\]와 같은 특정 네트워크의 경우 3 × 3 conolution 커널만
        사용

    -   커널 루프를 풀 수 있는 또 다른 이유는 Fast Convolution
        알고리즘으로 가속화를 달성

    -   4 × 4 피쳐 맵과 3 × 3 커널에 완전 병렬 주파수 영역 곱셈을 구현한

-   Lu et al.

    -   방정식 5 전용 파이프라인을 사용하여 FPGA에 Winograd 알고리즘을
        구현

    -   3 × 3 커널을 갖는 6 × 6 피쳐 맵의 컨볼루션은 완전히 병렬화

    -   위의 해결책들은 오직 하나의 층만을 위한 것

    -   특히 높은 병렬화가 필요한 경우 전체 네트워크를 위한 단일 규모의
        해결책은 거의 없음

    -   각 층이 파이프의 끝인 완전한 파이프 구조를 제안

        -   각 레이어는 하드웨어의 독립적인 부분으로 실행되고 각 부분은
            작기 때문에 루프 언롤링 방법을 쉽게 선택

        -   이 방법은 피쳐 맵에 대한 인접 레이어 사이에 ping-pong 버퍼가
            필요하기 때문에 메모리를 소비

        -   이항 가중치를 가진 능동적 설계는 FPGA에 더 잘 맞을 수 있음

        -   76\]의 설계는 유사하지만 확장성 문제를 해결하기 위해 FPGA
            클러스터에 구현

-   Shen et al. & Lin et al.

    -   루프의 트립 카운트로 CNN의 레이어를 그룹화하고 각 그룹을 하나의
        하드웨어 모듈에 매핑

    -   이러한 솔루션은 서로 다른 입력이 서로 다른 계층 파이프라인
        단계에서 병렬로 처리되기 때문에 배치 루프 언롤링과 같음

-   현재 설계의 대부분은 위의 루프 언롤링

    -   특별한 종류의 디자인은 특정 신경망을 위한 것

    -   Han et al

        -   희박한 LSTM 네트워크 가속을 위한 ESE 아키텍처를 제안

        -   밀도가 높은 네트워크를 처리하는 것과 달리, 모든 연산 장치는
            동시에 작동하지 않음

        -   이것은 서로 다른 계산 단위들 간에 데이터를 공유하는데
            어려움을 야기

        -   ESE는 하드웨어 설계를 단순화하고 배치 프로세스를 병렬화하기
            위해 레이어 내에서 출력 채널(LSTM의 FC 레이어의 출력 뉴런)
            루프만 구현

    1.  *Data Transfer and On-chip Memory Design*

-   온칩 메모리 시스템은 매 사이클마다 각 연산 장치에 필요한 데이터를
    효율적으로 제공해야 함

    -   높은 병렬 방식을 구현하기 위해 신경망 가속기는 대개 많은 수의
        연산 장치 사이에서 데이터를 재사용

    -   단순히 데이터를 다른 연산 장치에 방송하는 것은 큰 팬아웃과 높은
        라우팅 비용을 초래하여 작동 빈도를 감소

    -   Wei et al.

        -   수축기 배열 구조를 설계에 사용

        -   공유 데이터는 한 연산 장치에서 다음 연산 장치로 체인 모드로
            전송

        -   데이터는 방송되지 않고, 서로 다른 연산 장치 사이의 국부적인
            연결 필요

        -   단점은 대기 시간의 증가

        -   루프 실행 순서는 지연 시간을 포함하도록 그에 따라 스케줄링

-   GPU에서 소프트웨어를 구현하기 위해 im2col 함수는 일반적으로
    매트릭스-벡터 곱셈으로 2D 코로션을 매핑하는 데 사용

    -   상당한 데이터 중복성을 유발하며 FPGA의 제한된 온칩 메모리에 거의
        적용할 수 없음

        -   Qiu et al

            -   라인 버퍼 설계를 사용하여 중복 픽셀이 2개 라인인 2-d
                convolution에 대해 3 × 3 슬라이딩 윈도우 기능을 달성

2.  System Design

![스크린샷이(가) 표시된 사진 자동 생성된
설명](media/image14.png){width="6.26875in" height="2.932638888888889in"}

-   전형적인 FPGA 기반 신경망 가속기 시스템은 그림 4와 같음

    -   전체 시스템의 논리 부분은 파란 상자로 표시

    -   호스트 CPU는 FPGA 논리 부분에 워크로드 또는 명령을 내리고 작동
        상태를 모니터링

    -   FPGA 논리 부분에서는 일반적으로 컨트롤러를 구현하여 호스트와
        통신하고 FPGA의 다른 모든 모듈로 제어 신호를 생성

    -   컨트롤러는 FSM 또는 명령 디코더

    -   외부 메모리에서 로드된 데이터에 사전 처리가 필요한 경우 이동
        로직 파트는 특정 설계에 대해 구현

    -   이 모듈은 데이터 배열 모듈, 데이터 시프터\[49\], FFT 모듈 등이
        될 수 있음

    -   계산 단위는 섹션 5.1과 섹션 5.2에서 설명한 것과 같음

    -   제2.2절에 소개된 바와 같이, FPGA 칩의 온-칩 SRAM은 대형 NN
        모델에 비해 너무 제한적

    -   일반적인 설계의 경우 DDR 및 온-칩 메모리와 함께 2단계 메모리
        계층이 사용

    1.  *Roofline Model*

-   시스템 수준에서 신경망 가속기의 성능은 온칩 계산 자원과 오프칩
    메모리 대역폭이라는 두 가지 요인에 의해 제한

    -   특정 오프칩 메모리 대역폭 내에서 최고의 성능을 달성하기 위한
        다양한 연구가 제안

    -   Zhang et al.

    -   설계가 메모리 경계인지 계산 경계인지 분석하기 위해 작업에서
        roofline 모델을 소개

    -   루프라인 모델의 예는 그림 5와 같다.

![텍스트, 지도이(가) 표시된 사진 자동 생성된
설명](media/image15.png){width="6.26875in"
height="3.0145833333333334in"}

-   X축으로는 연산 대 통신(CTC)비를, Y축으로는 하드웨어 성능을 사용

    -   CTC는 메모리 액세스 단위 크기로 실행할 수 있는 작업 수

    -   각 하드웨어 설계는 그림에서 포인트로 취급

        -   따라서 y/x는 설계의 대역폭 요구량과 동일

    -   대상 플랫폼의 가용 대역폭은 제한

    -   그림 5에서 이론적 대역폭으로 설명

        -   그러나 DDR의 달성 가능한 대역폭은 데이터 액세스 패턴에 따라
            다르기 때문에 실제 대역폭 지붕은 이론적 대역폭 아래

        -   순차적 DDR 액세스는 랜덤 액세스보다 훨씬 높은 대역폭을 달성

        -   다른 지붕은 FPGA의 가용 자원에 의해 제한되는 계산

    1.  *Loop Tiling and Interchange*

-   CTC 비율이 높으면 하드웨어가 계산 바인딩을 달성할 가능성이 더 높음

    -   CTC 비율을 높이면 DDR 접속도 감소하여 에너지를 크게 절약

    -   루프 언롤링 전략이 결정되면, 루프의 나머지 부분의 스케줄링은
        하드웨어가 온칩 버퍼로 데이터를 재사용할 수 있는 방법을 결정

    -   이것은 루프 타일링과 루프 인터체인지 전략을 포함

-   루프 타일링은 더 높은 수준의 루프 언롤링

    -   루프 타일의 모든 입력 데이터는 칩에 저장

    -   루프 언롤링 하드웨어 커널은 이 데이터에서 작동

    -   루프 타일 크기가 크면 각 타일이 외부 메모리에서 온칩 메모리로 더
        적은 횟수로 로드된다는 것을 의미

    -   루프 교환 전략은 루프 타일의 처리 순서를 결정

    -   외부 메모리 액세스는 하드웨어가 한 타일에서 다른 타일로 이동할
        때 발생

    -   이웃 타일은 데이터의 일부를 공유

        -   예를 들어 CONV 레이어에서 인접 타일은 입력 피쳐 맵 또는
            가중치를 공유할 수 있음

        -   루프 실행 순서에 의해 결정

-   설계 공간 탐사는 가능한 모든 루프 타일 크기와 루프 순서에 따라 수행

    -   또한 많은 설계는 일부 루프를 unrolling, tiling 및 루프 순서로
        설계 공간을 탐색

    -   Shen et al.

        -   다른 계층에 대해 CTC에 대한 배치 평행성의 영향에 대해 논의

        -   이전 작품에서 초점이 맞춰지지 않은 루프 차원

-   위의 모든 작업은 하나의 최적화된 루프 언롤링 전략과 전체 네트워크에
    대한 루프 순서를 제공

    -   Guo et al.

        -   명령 인터페이스로 다른 레이어에 대해 유연한 언롤링 및 루프
            순서 구성을 구현

        -   온칩 버퍼의 데이터 배치는 다른 형상 지도 크기에 맞도록
            지침을 통해 제어

        -   이는 하드웨어가 온칩 버퍼 크기에 따라 가장 큰 타일링 크기를
            사용하기 위해 온칩 버퍼를 항상 충분히 활용할 수 있다는 것을
            의미

        -   이 작업은 또한 가장 안쪽 루프가 완료될 때 전체 온칩 데이터
            새로 고침을 방지하기 위해 \"뒤로\" 루프 실행 순서를 제안

    1.  *Cross-Layer Scheduling*

-   Alwani et al.

    -   두 계층 사이의 중간 결과 전송을 피하기 위해 두 인접 계층을 함께
        결합하여 외부 메모리 액세스 문제를 해결

    -   이 전략은 20% 추가 온칩 메모리 비용으로 95%의 오프칩 데이터
        전송을 줄이는 데 도움

    -   심지어 소프트웨어 프로그램도 이 스케줄링 전략으로 2배의 속도

-   Yu et al.

    -   명령 인터페이스를 통해 실행 순서를 수정하여 단층 가속기 설계에서
        이 아이디어를 실현

    1.  *Regularize Data Access Pattern*

-   실제 대역폭 지붕을 증가시키는 것은 특정 CTC 비율로 달성할 수 있는
    성능을 향상시키는 데 도움이는 DDR 액세스 패턴을 정규화함으로써 달성

    -   외부 메모리의 일반적인 형상 지도 형식은 NCHW 또는 CHW N

        -   N은 배치 치수를, C는 채널 치수를, H와 W는 형상 지도 y와 x
            치수를 의미

    -   형상 지도 타일을 불연속 주소에 저장된 작은 데이터 블록으로 자를
        수 있음

    -   Guan et al

        -   채널 주 저장 형식을 설계에 사용해야 한다고 제안

        -   이 형식은 긴 DDR 액세스 버스트가 보장되는 동안 데이터 복제를
            방지

    -   Qui et al

        -   H × W 피쳐 맵을 크기 r × c의 (HW /rc) 타일 블록으로 배열하는
            피쳐 맵 저장 형식을 제안

        -   쓰기 버스트 크기를 c/2에서 rc/2로 늘릴 수 있음

7.  Evaluation

-   최첨단 신경망 가속기 설계의 성능을 비교

    -   제4절과 제5절에 언급된 기법을 평가

    -   2015년부터는 상위 FPGA 컨퍼런스(FPGA, FCCM, FPL, FPT), EDA
        컨퍼런스(DAC, ASPDAC, DATE, ICCAD), 아키텍처 컨퍼런스(MICRO,
        HPCA, ISCA, ASPLOS)에 발표된 FPGA 기반 디자인을 주로 검토

    -   채택된 기법, 대상 FPGA 칩, 실험의 다양성 때문에, 비교의 공정성과
        우리가 사용할 수 있는 설계의 개수 사이의 절충이 필요

    -   본 논문에서는 1) 전체 시스템 구현과 2) 속도, 전력 및 에너지
        효율이 보고된 실제 NN 모델에 대한 실험을 통해 설계를 선정

-   비교에 사용되는 설계는 표 4에 수록

![텍스트이(가) 표시된 사진 자동 생성된
설명](media/image16.png){width="6.26875in" height="5.761111111111111in"}

-   데이터 형식의 경우 \"INT A/B\"는 활성화가 A비트 고정 소수점
    데이터이고 가중치는 B비트 고정 소수점 데이터임을 의미

-   자원 활용도를 조사하고 가속기 설계자와 FPGA 제조업체 모두에게 조언을
    구한다.

```{=html}
<!-- -->
```
-   표 4의 각 설계는 x 좌표로 log10(power)을 사용하고 y축으로
    log10(speed)을 사용하여 그림 6의 점으로 그림

![텍스트, 지도이(가) 표시된 사진 자동 생성된
설명](media/image17.png){width="6.261111111111111in"
height="7.634027777777778in"}

-   y - x = log10(energy\_efficiency)

-   사용된 GPU 실험 결과를 FPGA 설계의 성능을 측정하기 위한 표준으로
    표시

1.  Bit-width Reduction

-   1-2비트 기반 설계

    -   뛰어난 속도와 에너지 효율

    -   매우 낮은 비트 너비가 고성능 설계에 유망한 솔루션임

    -   제4.1절에 소개된 바와 같이, 정량화된 1-2비트 네트워크 모델은
        높은 정확도 loss

    -   관련 액셀러레이터를 더 개발하는 것은 거의 소용이 없음

    -   모델에 더 많은 노력을 기울여야 함

    -   현재의 하드웨어 성능을 고려할 때 정확한 거래 속도도 받아들일 수
        있음

-   32비트 부동 소수점 데이터 또는 8비트 이상의 선형 정량화를 채택한
    나머지 설계

    -   섹션 4.1의 결과에 따르면 1%의 정확도 손실을 달성

    -   이 디자인들 간의 비교가 정확하다고 생각

    -   INT16/8과 INT16은 일반적으로 채택

    -   이 디자인들의 차이는 명백하지 않음

        -   이는 5.1.1절에서 논의된 DSP의 활용도가 낮기 때문

    -   FP32에 대한 INT16의 장점은 하드코어 부동 소수점 DSP가 활용되는
        \[77\]을 제외하면 명백

    -   DPP 온칩을 완전히 활용하는 것의 중요성을 보여줌

    1.  Fast Convolution Algorithm

-   모든 16비트 설계 중에서 6 × 6 Winograd Fast Convolution을 사용해
    최고의 에너지 효율과 최고 속도를 달성

    -   32비트 부동 소수점 데이터와 28nm 기술 노드를 가진 FPGA를
        사용하는 것에 비해 2배의 속도 향상과 3배의 에너지 효율을 달성

    -   섹션 5.1.2에서 소개된 이론적 4배 성능 향상과 비교하면 여전히
        1.3 - 1.5배 차이가 있음

    -   모든 계층이 커널 크기 제한 때문에 가장 최적화된 Fast Convolution
        방법을 사용

    1.  System Level Optimization

-   전체 시스템 최적화

    -   동일한 XC7VX690T 플랫폼에서 세 가지 설계비교

        -   \[35\]가 가중치에 8비트를 사용하는 것을 제외하고 세 가지
            설계 모두 16비트 고정점 데이터 형식을 구현

        -   어떤 작업에도 빠른 경직이나 간격이 활용되지 않음

        -   \[30\]은 \[35\]의 2.5배의 에너지 효율을 달성

        -   시스템 수준 최적화가 Fast Convolution 알고리즘의 사용과
            비교할 수 있는 강력한 효과를 가지고 있음

-   표 4의 설계의 자원 활용도 조사

    -   자원(DSP, BRAM, 논리)의 3가지 종류가 고려

        -   이용률의 두 가지를 x와 y 좌표로 사용하여 그림 7에 설계를
            표시

![하늘, 실내이(가) 표시된 사진 자동 생성된
설명](media/image18.png){width="6.261111111111111in"
height="2.2090277777777776in"}

-   하드웨어 자원에 대한 설계의 선호도를 보여주기 위해 각 그림의 대각선
    표시

-   BRAM-DSP 수치는 BRAM보다 DSP를 분명히 선호

-   DSP에서 논리보다 비슷한 선호도

-   현재의 FPGA 설계가 더 가능성이 높음

-   설계자 중 일부는 높은 병렬성을 구현하기 위해 논리와 DSP를 모두
    사용하는 반면 일부는 높은 작동 빈도를 달성하기 위해 DSP만 사용하는
    것을 선호

1.  Comparison with GPU

-   FPGA 기반 디자인은 10-100GOP/J로 GPU와 비교 가능한 에너지 효율을
    달성

    -   GPU의 속도는 여전히 FPGA를 능가

    -   FPGA 기반 설계의 스케일 업은 여전히 문제

    -   Zhang et al.

        -   16비트 고정 소수점 계산을 이용한 FPGA 클러스터 기반 솔루션을
            제안

        -   에너지 효율은 다른 16비트 고정 소수점 설계보다 나쁨

-   이상적인 디자인의 실현 가능한 속도를 추정

    -   \[36\]의 16비트 고정점 설계를 베이스라인으로 사용

        -   이 설계는 최고 속도와 에너지 효율을 모두 갖춘 최고의 16비트
            고정점 설계

    -   섹션 4.1의 분석에 따라 8비트 선형 정량화를 채택

    -   1 DSP를 2개의 승수로 활용하여 또 다른 2배의 속도 향상과 더 나은
        에너지 효율을 달성

        -   이중 주파수 최적화는 시스템 속도를 2배 향상

    -   10%가 아닌 값과 유사한 희소 모형을 고려

        -   우리는 \[19\]와 비슷한 6배의 개선을 추정할 수 있음

        -   일반적으로 약 24배의 속도 향상과 12배의 에너지 효율을 달성할
            수 있음

        -   이는 약 50W로 72TOP/s의 속도를 의미

        -   GPU에서 32비트 부동 소수점 프로세스에서 FPGA에서 10배 이상의
            에너지 효율을 달성할 수 있음

-   남은 문제는 더블 MAC, Sparsification, 정량화, Fast Convolution, 더블
    주파수 설계 등 모든 기법이 잘 통하느냐는 것

    -   2D 커널은 항상 전체적으로 처리되기 때문에, 1개의 요소를 2D
        컨볼루션 커널에서 프루닝하는 것은 Fast Convolution에는 소용이
        없음

        -   전체적으로 2D 커널을 직접 자르는 것이 도움이 될 수 있음

        -   그러나 이 방법의 보고된 정확도는 미세하게 다듬은 프루닝보다
            낮음

        -   희박한 네트워크를 처리하기 위한 불규칙한 데이터 액세스
            패턴과 병렬 방식의 증가도 메모리 시스템 설계와 스케줄링
            전략에 도전을 가져옴

8.  Technique Discussion

-   각 기법의 하드웨어 설계에 영향을 미치는 방법과 NN 모델과 관련된
    정도의 두 가지 측면에서 평가

![지도, 텍스트이(가) 표시된 사진 자동 생성된
설명](media/image19.png){width="6.261111111111111in" height="3.4625in"}

9.  Design Automation and Flexibility

-   CNN 가속기의 설계 자동화

    -   Venieris et al.

        -   지원되는 모델, 인터페이스, 하드웨어 아키텍처, 설계 공간 탐색
            및 산술 정밀도의 다양한 도구 흐름에 대해 자세히 설명

    -   툴 흐름을 하드웨어 설계 자동화 및 소프트웨어 설계 자동화라는 두
        가지 범주로만 분류

    -   하드웨어 설계 자동화는 서로 다른 NN 모델에 따라 다른 하드웨어
        설계를 생성

    -   소프트웨어 설계 자동화는 동일한 가속기를 유지, 가속기에 다른
        입력을 발생

    1.  Hardware Design Automation

    -   하드웨어 설계 자동화

        -   제안된 기법들은 네트워크 매개변수에 기초한 HDL 설계를
            자동으로 생성하는 데 초점

        -   이들 방법의 차이는 높은 수준의 네트워크 설명과 낮은 수준의
            하드웨어 설계 사이의 간격을 커버하기 위한 네트워크의 중간
            수준 기술 선택

    -   입력 네트워크 설명과 플랫폼 제약조건으로 수작업으로 제작한
        Verilog 템플릿에 대해 최적화된 매개변수를 검색

        -   이 방법은 섹션 5에서 언급한 최적화 방법과 유사

        -   DiCecco et al.

            -   OpenCL 모델에 기초한 유사한 아이디어를 사용

            -   개발 도구가 카페와 통합되고 하나의 네트워크가 다른
                플랫폼에서 실행될 수 있게 함

        -   Venireis, et al.

            -   네트워크 모델을 설계 도구의 DFG로 설명

            -   네트워크 연산 프로세스는 DFG 매핑 방법으로 하드웨어
                설계로 변환

        -   DnWeaver et al.

            -   네트워크를 설명하기 위해 가상 명령 집합을 사용

            -   네트워크 모델은 먼저 명령 시퀀스로 변환

            -   그런 다음 이 시퀀스는 하드웨어 FSM 상태로 매핑되지만
                기존의 CPU 명령처럼 실행되지는 않음

    -   하드웨어 설계 자동화는 서로 다른 네트워크를 지원하도록 하드웨어
        설계를 직접 수정

        -   이것은 하드웨어가 항상 목표 플랫폼에서 최고의 성능을 달성할
            수 있다는 것을 의미

        -   이것은 FPGA의 재구성 가능성 때문에 적합

        -   네트워크 전환이 빈번하지 않고 재구성 오버헤드가 신경 쓰지
            않는 상황에서 작동

        -   대규모 클라우드 서비스의 경우, 네트워크 모델의 변화는 서로
            다른 FPGA 칩 사이에서 전환함으로써 커버

            -   FPGA를 자주 재구성할 필요는 없음

    2.  Software Design Automation

-   소프트웨어 설계 자동화

    -   대부분의 경우 명령 시퀀스를 간단히 입력만 변경함으로써 동일한
        하드웨어 가속기에서 서로 다른 네트워크를 실행

    -   이 작업들 간의 차이는 세부적인 트레이닝

    -   Gue et al.

        -   LOAD, CALC, SAVE의 세 가지 지시사항만을 가지고 지시사항을
            제안

        -   LOAD 및 SAVE 지침의 세분성은 데이터 타일링 크기와 동일

        -   각 CONV는 지침에 인코딩된 형상 지도 크기를 고려하여 2D
            코볼루션 세트를 실행

        -   채널 번호는 하드웨어 언롤링 파라미터로 고정

        -   이 수준에서 소프트웨어 컴파일러는 각 계층에 따라 정적
            스케줄링 및 동적 데이터 재사용 전략을 수행

    -   DNNDK et al.

        -   유사한 아이디어를 사용하지만 다양한 네트워크를 지원

    -   Zhang et al.

        -   계층 수준 명령 집합을 사용

        -   CNN 레이어의 제어는 구성 가능한 하드웨어 FSM으로 설계

        -   지시를 위한 메모리 액세스를 줄이는 동시에 구성 가능한 FSM의
            하드웨어 비용을 증가

-   TVM

    -   CPU, GPU, FPGA, ASIC를 포함한 다양한 종류의 플랫폼에 대해 균일한
        매핑 최적화 프레임워크를 구현

    -   이 프레임워크를 통해 개발자들은 FPGA 가속기를 포함한 맞춤형
        하드웨어를 지원하기 위해 맞춤화된 병렬 원시성을 정의 가능

    -   세부적인 일정을 보다 유연하게 잡을 수 있다는 것을 의미

-   명령 기반 방법은 하드웨어를 수정하지 않으므로 가속기가 런타임에
    네트워크 사이를 전환 가능

    -   애플리케이션 시나리오의 예로는 모바일 플랫폼의 실시간 비디오
        처리 시스템을 들 수 있음

    -   작업이 충분히 복잡할 경우 단일 프레임의 프로세스는 서로 다른
        네트워크를 포함할 수 있음

    -   하드웨어를 재구성하면 허용 불가능한 오버헤드가 발생하는 반면,
        지침 기반 방법은 모든 네트워크의 지침을 메모리에 준비하면 문제를
        해결할 수 있음

    1.  Mixed Method

-   Wang et al.

    -   하드웨어 설계를 최적화, 소프트웨어 지침을 컴파일함으로써 위의 두
        가지를 혼합, 설계 자동화 프레임워크를 제안

    -   하드웨어는 먼저 최적화된 하드웨어 매개변수를 사용하여 미리
        정의된 HDL 템플릿으로 조립

    -   연산 프로세스의 데이터 제어 흐름은 네트워크 설명에 따라 컴파일
        되는 소프트웨어 바이너리에 의해 제어

    -   하드웨어는 소프트웨어 바이너리를 변경하기만 하면 새로운
        네트워크에 사용될 수 있다.

10. Conclusion

-   본 논문에서는 최첨단 신경망 가속기 설계를 검토하고, 사용된 기법을
    정리

    -   6절의 평가 결과에 따르면, 소프트웨어 하드웨어 공동 설계를 통해
        FPGA는 최첨단 GPU보다 10배 이상의 속도와 에너지 효율을 달성할 수
        있음

        -   FPGA가 신경망 가속화에 유망한 후보임을 보여줌

    -   또한 현재의 개발 흐름이 고성능 네트워크 스위치와 런타임 네트워크
        스위치를 모두 달성할 수 있음을 보여주는 가속기 설계 자동화에
        사용되는 방법을 검토

-   현재 디자인과 추정 사이에는 여전히 차이가 있음

    -   극도로 좁은 비트 폭의 정량화는 모델 정확도에 의해 제한, 추가적인
        알고리즘 연구가 필요

    -   모든 기술을 결합하면, 소프트웨어와 하드웨어가 함께 잘 작동하도록
        하기 위해 더 많은 연구가 필요

    -   DNNDK\[3\]를 포함한 상업용 도구는 첫발을 내딛고 있음

    -   설계 규모를 늘리는 것도 문제
